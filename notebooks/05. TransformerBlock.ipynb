{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Central Component of GPT Model\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "225ec960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 128\n",
      "    num_heads = 8\n",
      "    head_dim = 16\n",
      "    Generating nn.Linear(768, 128) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "=== End Initialization ===\n",
      "\n",
      "\n",
      "\n",
      "----- Using real data input (see DataPreparation.ipynb) --\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=8, head_dim=16, output_dim=128\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 128])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 8, 16])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 8, 4, 16])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 8, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.2500\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 9.8689e-01,  1.5709e-01,  5.2774e-01,  3.7074e-01],\n",
      "        [-2.9761e+00,  2.7980e-01, -2.1499e-01,  3.0631e+00],\n",
      "        [ 1.2814e+00,  1.2523e+00,  2.7011e+00, -7.7821e-04],\n",
      "        [-3.8898e-01,  9.9227e-01,  2.0401e-01,  3.0814e+00]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 0.9869,    -inf,    -inf,    -inf],\n",
      "        [-2.9761,  0.2798,    -inf,    -inf],\n",
      "        [ 1.2814,  1.2523,  2.7011,    -inf],\n",
      "        [-0.3890,  0.9923,  0.2040,  3.0814]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 8, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3070, 0.6930, 0.0000, 0.0000],\n",
      "        [0.2925, 0.2904, 0.4171, 0.0000],\n",
      "        [0.1680, 0.2372, 0.1948, 0.4000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1.], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3838, 0.8662, 0.0000, 0.0000],\n",
      "        [0.3656, 0.3630, 0.5214, 0.0000],\n",
      "        [0.2100, 0.2966, 0.2435, 0.5000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 8, 16])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([ 1.2020,  0.8138,  0.1125,  0.1420, -0.0517,  0.3173, -1.8397, -0.9432,\n",
      "        -0.1389,  1.6354,  0.6562, -0.8412,  1.0339, -0.9603,  1.5243,  0.2921],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 128])\n",
      "   First concatenated context vector (batch 0, token 0): tensor([ 1.2020,  0.8138,  0.1125,  0.1420, -0.0517,  0.3173, -1.8397, -0.9432,\n",
      "        -0.1389,  1.6354,  0.6562, -0.8412,  1.0339, -0.9603,  1.5243,  0.2921,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         1.7353,  0.4753, -2.1176,  0.4204, -0.5770,  1.1375, -0.5424, -0.8481,\n",
      "        -0.0430, -0.3872, -0.7311, -1.5362, -0.2595,  1.5613,  0.3065,  0.4686,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.1892, -2.0398, -0.7310, -1.1861, -1.3698, -0.3969,  2.1243, -0.1451,\n",
      "         0.4116, -0.3931, -0.4757, -0.0471,  0.0297,  0.7218, -1.9437, -1.1326,\n",
      "         0.1148, -0.3320,  0.3840,  0.0737, -0.0150, -0.2663, -1.6948, -0.3815,\n",
      "         0.5604,  1.5224,  1.0008,  0.2064,  0.1108, -0.0609, -1.3077, -1.6534],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 128])\n",
      "   Final context vector (batch 0, token 0): tensor([ 0.0067,  0.1619,  1.1435,  0.7072, -0.7486,  0.6573, -0.1626, -0.3244,\n",
      "         0.5068, -0.0166, -0.0610,  0.3889,  0.0783,  0.4400,  0.6432, -0.2443,\n",
      "        -0.6105, -0.6596,  0.1916, -0.1415, -0.5645, -0.1697, -0.0360, -0.0408,\n",
      "         0.1512, -0.1578,  0.4119, -0.0247,  0.4899,  0.5286, -0.3619,  0.1185,\n",
      "         0.3404,  0.0827, -0.1494,  0.1649, -0.1070,  0.1966, -0.5404, -0.2402,\n",
      "         0.6727,  0.1224,  0.2570, -0.0547,  0.1806,  0.0692,  0.6536, -0.7658,\n",
      "        -0.0797,  0.2564, -0.1324, -0.0870,  0.1398,  0.1496, -0.0818, -0.2543,\n",
      "        -0.3107,  0.1479, -0.3162,  0.4159, -0.2524, -0.2391, -0.1158, -0.2504,\n",
      "        -0.3283, -0.1371,  0.3052,  0.5194, -0.0813,  0.3079,  0.1968,  0.7248,\n",
      "         0.4668, -0.5753, -0.9502, -0.4340,  0.3416,  0.3270, -1.0362,  0.1704,\n",
      "         0.2531, -0.3697,  0.6193,  0.0968, -0.3090, -0.1135,  0.3695,  0.0045,\n",
      "         0.4712,  0.4979, -0.4673, -0.6364,  0.4005, -0.4416,  0.6769, -0.6666,\n",
      "         0.1951,  0.7035, -0.0850,  0.0967,  0.1873, -0.1527,  0.5867, -0.2968,\n",
      "         0.2900,  0.4079,  0.6444, -0.0494, -0.0521,  0.5826,  0.0697, -0.2265,\n",
      "         0.6078, -0.0838,  0.5353,  0.0986,  0.4727, -0.0426, -0.3527,  0.2215,\n",
      "        -0.6271, -0.5469, -0.1399, -0.4259, -0.4475, -0.0896, -0.0930, -0.5143],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End Forward Pass ===\n",
      "\n",
      "Embbed_dim:  6\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  6\n",
      "    Hidden dimension =  24\n",
      "=== End Initialization ===\n",
      "\n",
      "Sample data:  tensor([[[0.4234, 0.6038, 0.1525, 0.3970, 0.8703, 0.7563],\n",
      "         [0.1836, 0.0991, 0.1583, 0.0066, 0.1142, 0.3764],\n",
      "         [0.8374, 0.5837, 0.1197, 0.0989, 0.7487, 0.1281]],\n",
      "\n",
      "        [[0.4384, 0.7399, 0.2686, 0.4455, 0.4565, 0.3817],\n",
      "         [0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585],\n",
      "         [0.1642, 0.6212, 0.6378, 0.7740, 0.8801, 0.7784]]])\n",
      "\n",
      "Output shape  torch.Size([2, 3, 6])\n",
      "Output data  tensor([[[-0.2428,  0.2256,  0.1109, -0.0539,  0.3551,  0.1985],\n",
      "         [-0.1200,  0.1231,  0.0985, -0.0327,  0.2840,  0.1089],\n",
      "         [-0.2068,  0.2289,  0.0662, -0.0584,  0.3875,  0.1665]],\n",
      "\n",
      "        [[-0.2002,  0.2058,  0.1277, -0.0006,  0.3472,  0.2213],\n",
      "         [-0.1778,  0.2566,  0.1017, -0.0911,  0.3864,  0.1499],\n",
      "         [-0.2514,  0.2692,  0.1715, -0.0407,  0.3557,  0.2312]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%run \"02. MultiHeadAttention.ipynb\"\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"04. FeedForward.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm\n",
    "FeedForward = FeedForward\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a3953f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if verbose: print(f\"\\n=== Transformer Initialization ===\")\n",
    "        \n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.num_heads = cfg[\"n_heads\"]\n",
    "        self.dropout_rate = cfg[\"drop_rate\"]\n",
    "        self.qkv_bias = cfg[\"qkv_bias\"]\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            input_dim = self.embbed_dim,\n",
    "            output_dim = self.embbed_dim,\n",
    "            context_length = self.context_length,\n",
    "            dropout = self.dropout_rate,\n",
    "            num_heads = self.num_heads,\n",
    "            qkv_bias = self.qkv_bias,\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(self.embbed_dim, verbose=verbose)\n",
    "        self.norm1 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.norm2 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        print(\"Dropout rate: \", self.dropout_rate)\n",
    "\n",
    "        if verbose: print(f\"\\n=== End Transformer Initialization ===\")\n",
    "        \n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "\n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== TransformerBlock Forward Pass ===\")\n",
    "            print(f\"Input shape: {x.shape} (batch_size={batch_size}, context_length={context_length}, input_dim={input_dim})\")\n",
    "            print(f\"Config: num_heads={self.num_heads}, embbed_dim={self.embbed_dim}\")\n",
    "            print(f\"\\nInput tensor (batch 0 with shape {x[0].shape}) also stored as shortcut:\")\n",
    "            print(f\" {x[0]}\")\n",
    "            \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if verbose: print(f\"\\n1. Normalization:\\n {x[0]}\")\n",
    " \n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377056e1",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46ade3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedder Initialization ===\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Transformer Initialization ===\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 768\n",
      "    num_heads = 12\n",
      "    head_dim = 64\n",
      "    Generating nn.Linear(768, 768) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(1024, 1024), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.1) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(768, 768) weights for final context_vector projection\n",
      "=== End Initialization ===\n",
      "\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  768\n",
      "    Hidden dimension =  3072\n",
      "=== End Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End Initialization ===\n",
      "\n",
      "Dropout rate:  0.1\n",
      "\n",
      "=== End Transformer Initialization ===\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])) also stored as shortcut:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. Normalization:\n",
      " tensor([[-0.1554, -1.4885,  1.1975,  ...,  1.4838, -1.3720,  0.6351],\n",
      "        [-2.2864, -0.5368, -0.5850,  ..., -1.5195, -0.3160,  0.8553],\n",
      "        [-0.5828, -1.3211, -1.0055,  ...,  1.0229, -0.0117,  1.4315],\n",
      "        [ 0.1354,  0.1240, -0.2650,  ...,  1.7749, -0.1572,  0.2964]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output shape:  torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "def use_transformer_block(verbose = False):\n",
    "\n",
    "    %run \"01. DataPreparation.ipynb\"\n",
    "    input = get_test_input_embedding(verbose=verbose)\n",
    "\n",
    "    block = TransformerBlock(cfg = GPT_CONFIG_124M, verbose=verbose)\n",
    "    y = block(input, verbose=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Output shape: \", y.shape)\n",
    "\n",
    "_test_run = use_transformer_block(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35223b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

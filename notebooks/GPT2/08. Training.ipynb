{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5dcba",
   "metadata": {},
   "source": [
    "# Playground for Training\n",
    "* Train, Save and Load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285f56",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49d2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3, precision=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"01. DataPreparation.ipynb\"\n",
    "%run \"06. GPTModel.ipynb\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "# From 01. DataPreparation\n",
    "create_dataloader = create_dataloader\n",
    "\n",
    "# From 06. GPTModel\n",
    "model = GPTModel(GPT_CONFIG_124M) # From 06. GPTModel.ipynb\n",
    "model.eval();  # Disable dropout during inference\n",
    "generate_tokens = generate_tokens # From 06. GPTModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d79d",
   "metadata": {},
   "source": [
    "## Creating Training and Validation Data\n",
    "* Generates test Batches Input + Targets\n",
    "* Drop_last removes incomplete last batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2b01812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Institute of Computational Excellence did not believe in budgets. It believed in innovation, ca  ... \n",
      "Characters: 24733\n",
      "Tokens: 6746\n",
      "\n",
      "Input: ([Batches, Context_Length]) Target: ([Batches, Context_Length])\n",
      "\n",
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "def get_training_data(batch_size=2, context_length=4, stride=4, verbose=False):\n",
    "    \n",
    "    # Load sample text and create dataloader\n",
    "    with open(\"00. Robins Small Text Sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read()\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    print(raw_text[:99], \" ... \")\n",
    "    total_characters = len(raw_text)\n",
    "    total_tokens = len(tokenizer.encode(raw_text))\n",
    "    print(\"Characters:\", total_characters)\n",
    "    print(\"Tokens:\", total_tokens)\n",
    "\n",
    "    # Train/validation ratio\n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(raw_text))\n",
    "    train_data = raw_text[:split_idx]\n",
    "    val_data = raw_text[split_idx:]\n",
    "\n",
    "\n",
    "    train_loader = create_dataloader(\n",
    "        train_data,\n",
    "        batch_size=2,\n",
    "        context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader(\n",
    "        val_data,\n",
    "        batch_size=2,\n",
    "        context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    train_loader, val_loader = get_training_data(verbose=True)\n",
    "    \n",
    "    print(\"\\nInput: ([Batches, Context_Length]) Target: ([Batches, Context_Length])\")\n",
    "    print(\"\\nTrain loader:\")\n",
    "    for x, y in train_loader:\n",
    "        print(x.shape, y.shape)\n",
    "\n",
    "    print(\"\\nValidation loader:\")\n",
    "    for x, y in val_loader:\n",
    "        print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7ce61",
   "metadata": {},
   "source": [
    "## Determine Device and set model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b810aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if '__file__' not in dir():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using {device} device.\")\n",
    "\n",
    "    # Move model to device BEFORE using it\n",
    "    model.to(device) #Add ; to remove model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc98b60",
   "metadata": {},
   "source": [
    "## Helper\n",
    "* `calc_loss_batch` Calculates cross-entropy loss on model output for given input and target batch\n",
    "* `calc_loss_loader` Calculates average of cross-entropy loss over all batches in a dataloader\n",
    "* `text_to_token_ids` Simply converty text into Token_IDS (adds batch dimension)\n",
    "* `token_ids_to_text` Simply converty token_IDs into text (removes batch dimension)\n",
    "* `evaluate_model` Used for training output. Sets model to eval and calculates loss on train and val data\n",
    "* `generate_and_print_sample` Used for training output. Generates more tokens given a starting context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "32651044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.008652600375088\n",
      "Validation loss: 11.06159782409668\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_tokens(\n",
    "            model=model, token_IDs=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, None)\n",
    "\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef108abc",
   "metadata": {},
   "source": [
    "## Training\n",
    "* Performs training and outputs some evaluation results (see helper above)\n",
    "* Untrained models tend to tokens that occurr often (bias) so first outputs are (., \", ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "267f2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.193, Val loss 9.405\n",
      "Ep 1 (Step 000005): Train loss 7.570, Val loss 7.976\n",
      "Ep 1 (Step 000010): Train loss 6.071, Val loss 7.336\n",
      "They brewed coffee. “” ” ” ” ” ” ” ” ” ” ” ” ” ” ” ” \n",
      "Ep 2 (Step 000015): Train loss 5.604, Val loss 7.027\n",
      "Ep 2 (Step 000020): Train loss 5.190, Val loss 6.952\n",
      "They brewed coffee. “We” ” ” ” ” ” “I” ” ” ” “We” ” ” �\n",
      "Ep 3 (Step 000025): Train loss 4.579, Val loss 7.079\n",
      "Ep 3 (Step 000030): Train loss 4.177, Val loss 6.873\n",
      "They brewed coffee. Sebastian” Sebastian” ” ” Sebastian” Sebastian“I. ” ” “I”  \n",
      "Ep 4 (Step 000035): Train loss 3.261, Val loss 6.907\n",
      "Ep 4 (Step 000040): Train loss 3.179, Val loss 6.925\n",
      "They brewed coffee. Sebastian. “We’s voice filled the lab.” Sebastian, the mug.” Sebastian eyed the mug.” “We” �\n",
      "Ep 5 (Step 000045): Train loss 2.715, Val loss 6.941\n",
      "Ep 5 (Step 000050): Train loss 2.217, Val loss 6.954\n",
      "They brewed coffee. Sebastian placed. Thorne.” “Patchwork?” Robin asked. They smiled. “I am to a a. “We” ” The dean,”\n",
      "Ep 6 (Step 000055): Train loss 1.836, Val loss 6.901\n",
      "Ep 6 (Step 000060): Train loss 1.452, Val loss 6.938\n",
      "Ep 6 (Step 000065): Train loss 1.208, Val loss 6.983\n",
      "They brewed coffee. Sebastian took the mug. Sebastian took the lab. “I. Sebastian the shard. “I am just a story about myself.“I am the board. \n",
      "Ep 7 (Step 000070): Train loss 0.988, Val loss 6.985\n",
      "Ep 7 (Step 000075): Train loss 0.773, Val loss 7.055\n",
      "They brewed coffee.“I can hummed.” “Actual utility,” Sebastian said, faintly surprised. “I am.” “I am just a story about myself.” ” “\n",
      "Ep 8 (Step 000080): Train loss 0.726, Val loss 7.172\n",
      "Ep 8 (Step 000085): Train loss 0.598, Val loss 7.198\n",
      "They brewed coffee. “You’s voice previous model had: it refused glory. It produced sentences that could survive Tuesday. “They smiled wider.” “This goes in the brochure.” Robin said.” Robin\n",
      "Ep 9 (Step 000090): Train loss 0.497, Val loss 7.183\n",
      "Ep 9 (Step 000095): Train loss 0.371, Val loss 7.219\n",
      "They brewed coffee. No one remembered Hyperscience. Not the dean, not the government, not the world. Except Robin and Sebastian. They stood in the quiet lab, surrounded by inert machines and empty coffee cups. TheUniverse trended globally\n",
      "Ep 10 (Step 000100): Train loss 0.291, Val loss 7.289\n",
      "Ep 10 (Step 000105): Train loss 0.268, Val loss 7.358\n",
      "They brewed coffee. ” the dean shouted. “I’m afraid that feature is deprecated.”  Within a week, Hyperscience went viral. Hashtags like #RealityPatch and #CodeTheUniverse trended globally\n"
     ]
    }
   ],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    track_tokens_seen = []\n",
    "    tokens_seen = 0 \n",
    "    global_step = -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode  # good to always set flag\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "        start_context=\"They brewed coffee.\", tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52577b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

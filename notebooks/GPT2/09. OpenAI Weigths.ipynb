{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5dcba",
   "metadata": {},
   "source": [
    "# Playground for OpenAI weights\n",
    "* Loading and trying openai weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285f56",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49d2549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3, precision=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%run \"06. GPTModel.ipynb\"\n",
    "%run \"08. Training.ipynb\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "       # Vocabulary size\n",
    "    \"vocab_size\": 50257,\"context_length\": 1024, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": True       # Query-key-value bias -- is true for real gpt\n",
    "}\n",
    "\n",
    "# From 06. GPTModel\n",
    "GPTModel = GPTModel # From 06. GPTModel.ipynb\n",
    "generate_tokens = generate_tokens # From 06. GPTModel.ipynb\n",
    "\n",
    "# from 08. Training\n",
    "text_to_token_ids = text_to_token_ids \n",
    "token_ids_to_text = token_ids_to_text\n",
    "\n",
    "# determine hardware device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f16517",
   "metadata": {},
   "source": [
    "## Load Pre-trained GPT-2 Model from HuggingFace\n",
    "* Downloads pre-trained PyTorch weights (.pth files) if not present locally\n",
    "* Available sizes: 124M (small), 355M (medium), 774M (large), 1558M (xl)\n",
    "* Loads weights into model and generates sample text for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e45f6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you and my team around, so please donate today and consider donating before Labor Day. The money will go toward further research and development\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# file_name = \"gpt2-small-124M.pth\"\n",
    "# file_name = \"gpt2-medium-355M.pth\"\n",
    "# file_name = \"gpt2-large-774M.pth\"\n",
    "# file_name = \"gpt2-xl-1558M.pth\"\n",
    "def load_model_from_scratch(file_name = \"gpt2-small-124M.pth\", config = GPT_CONFIG_124M):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        urllib.request.urlretrieve(url, file_name)\n",
    "        print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "    gpt = GPTModel(config)\n",
    "    gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "    gpt.eval()\n",
    "    gpt.to(device);\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    token_ids = generate_tokens(\n",
    "        model=gpt,\n",
    "        token_ids=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=config[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "\n",
    "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    load_model_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db71b69",
   "metadata": {},
   "source": [
    "## Manual Weight Loading from Original OpenAI GPT-2 Checkpoint\n",
    "* Downloads 7 checkpoint files from OpenAI's official blob storage\n",
    "* Extracts weights from TensorFlow checkpoint format\n",
    "* Manual weight assignment to match custom architecture naming:\n",
    "  - Token/position embeddings (wte/wpe)\n",
    "  - Attention QKV matrices with bias splitting\n",
    "  - Feed-forward layers (MLP)\n",
    "  - Layer normalization parameters (scale/shift)\n",
    "* Verifies loaded model with text generation\n",
    "* File downloading code for GPT. is in `gpt_download.py`. Just skip no magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0698c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "\n",
      "Example: wte - Token embedding weight tensor dimensions: (50257, 768)\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "\n",
      "Output text:\n",
      " Every effort moves you and my team around, so please donate today and consider donating before Labor Day. The money will go toward further research and development\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"vocab_size\": 50257, \"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12, \"context_length\": 1024, \"drop_rate\": 0.1, \"qkv_bias\": True},\n",
    "    \"gpt2-medium (355M)\": {\"vocab_size\": 50257, \"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16, \"context_length\": 1024, \"drop_rate\": 0.1, \"qkv_bias\": True},\n",
    "    \"gpt2-large (774M)\": {\"vocab_size\": 50257, \"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20, \"context_length\": 1024, \"drop_rate\": 0.1, \"qkv_bias\": True},\n",
    "    \"gpt2-xl (1558M)\": {\"vocab_size\": 50257, \"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25, \"context_length\": 1024, \"drop_rate\": 0.1, \"qkv_bias\": True},\n",
    "}\n",
    "\n",
    "# Inits model base structure\n",
    "def init_model(model_name):\n",
    "    config = model_configs[model_name]\n",
    "    gpt = GPTModel(config)\n",
    "    gpt.eval();\n",
    "    return gpt\n",
    "\n",
    "# Assign function for weight matrices\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "# Manually loading weitghts into gpt\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "    print(\"\\nSettings:\", settings)\n",
    "    print(\"\\nParameter dictionary keys:\", params.keys())\n",
    "\n",
    "    print(\"\\nExample: wte - Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\n",
    "    print(params[\"wte\"])\n",
    "    \n",
    "    gpt = init_model(\"gpt2-small (124M)\")\n",
    "    load_weights_into_gpt(gpt, params)\n",
    "    gpt.to(device);\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    token_ids = generate_tokens(\n",
    "        model=gpt,\n",
    "        token_ids=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=1024,\n",
    "        top_k=50,\n",
    "        temperature=1.5\n",
    "    )\n",
    "\n",
    "    print(\"\\nOutput text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f38d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

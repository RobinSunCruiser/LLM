{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Central Component of GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225ec960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 128\n",
      "    num_heads = 8\n",
      "    head_dim = 16\n",
      "    Generating nn.Linear(768, 128) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "\n",
      "----- Using real data input (see DataPreparation.ipynb) --\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([1722, 1528, 2900,  656]) As days turned into\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([1528, 2900,  656, 2745])  days turned into weeks\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.6743,  0.2851,  0.5607,  ..., -0.3712,  0.6174, -0.5773],\n",
      "        [ 0.9675,  0.3970,  0.0029,  ...,  0.7599,  0.8818, -0.1195],\n",
      "        [-1.1204,  1.5178, -0.5717,  ...,  0.1642, -0.5433,  0.8593],\n",
      "        [ 0.5127, -0.5098,  0.7294,  ..., -0.8692, -0.6278, -0.3639]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[-0.3834,  1.2171,  0.0605,  ..., -0.9916,  0.5717,  0.5908],\n",
      "        [-0.0440,  0.6047, -0.7804,  ..., -1.8195,  2.0859, -0.1849],\n",
      "        [ 0.0617,  3.1600,  0.1060,  ..., -0.7261,  0.8990,  0.2640],\n",
      "        [ 1.0921, -1.5200, -1.1416,  ..., -0.8093,  1.4341, -2.5053]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=8, head_dim=16, output_dim=128\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 128])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 8, 16])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 8, 4, 16])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 8, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.2500\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 1.0885,  4.3496, -1.3913, -1.0626],\n",
      "        [ 2.0139, -2.9493,  0.5475,  3.7845],\n",
      "        [ 0.7977, -0.8061, -4.0471, -4.1537],\n",
      "        [ 1.9989,  0.7105, -3.0913, -3.9818]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 1.0885,    -inf,    -inf,    -inf],\n",
      "        [ 2.0139, -2.9493,    -inf,    -inf],\n",
      "        [ 0.7977, -0.8061, -4.0471,    -inf],\n",
      "        [ 1.9989,  0.7105, -3.0913, -3.9818]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 8, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7757, 0.2243, 0.0000, 0.0000],\n",
      "        [0.5083, 0.3404, 0.1514, 0.0000],\n",
      "        [0.4486, 0.3251, 0.1257, 0.1006]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9696, 0.2804, 0.0000, 0.0000],\n",
      "        [0.6353, 0.4255, 0.1892, 0.0000],\n",
      "        [0.5608, 0.4064, 0.1571, 0.0000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 8, 16])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 128])\n",
      "   First concatenated context vector (batch 0): tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0666,  2.3536,  0.3662],\n",
      "        [ 0.2478, -0.5788,  0.8975,  ...,  0.0994,  1.8833,  0.2843],\n",
      "        [ 0.3497, -0.7968,  0.6128,  ...,  0.0634, -0.2996,  0.3811],\n",
      "        [ 0.3290, -0.7510,  0.5758,  ..., -0.4330,  0.4884,  0.1517]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 128])\n",
      "   Final context vector (batch 0): tensor([[-0.1452, -0.0858,  0.5867,  ...,  0.5176,  0.3168,  0.1959],\n",
      "        [-0.1013, -0.4879, -0.2271,  ...,  0.8702,  0.1652, -0.7102],\n",
      "        [-0.2722,  0.1297,  0.1583,  ...,  0.3138,  0.1148, -0.6699],\n",
      "        [ 0.1073,  0.0704,  0.0414,  ...,  0.0026, -0.1856, -0.1136]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "Embbed_dim:  6\n",
      "Test input\n",
      ": tensor([[0.2483, 0.0000, 0.0000, 0.4067, 0.4628, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5766, 1.7535, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1863],\n",
      "        [0.3884]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0466],\n",
      "        [0.5005]], grad_fn=<VarBackward0>)\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 6\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(6))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(6))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Forward Pass ===\n",
      "    Input (first 5 elements):  tensor([0.2483, 0.0000, 0.0000, 0.4067, 0.4628], grad_fn=<SliceBackward0>)\n",
      "    Normalized (norm_x first 5 elements):  tensor([ 0.3144, -0.9452, -0.9452,  1.1184,  1.4030], grad_fn=<SliceBackward0>)\n",
      "    Output = self.scale * norm_x + self.shift\n",
      "=== End LayerNorm Forward Pass ===\n",
      "\n",
      "Embbed_dim:  6\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  6\n",
      "    Hidden dimension =  24\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "Sample data:  tensor([[[0.4234, 0.6038, 0.1525, 0.3970, 0.8703, 0.7563],\n",
      "         [0.1836, 0.0991, 0.1583, 0.0066, 0.1142, 0.3764],\n",
      "         [0.8374, 0.5837, 0.1197, 0.0989, 0.7487, 0.1281]],\n",
      "\n",
      "        [[0.4384, 0.7399, 0.2686, 0.4455, 0.4565, 0.3817],\n",
      "         [0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585],\n",
      "         [0.1642, 0.6212, 0.6378, 0.7740, 0.8801, 0.7784]]])\n",
      "\n",
      "Output shape  torch.Size([2, 3, 6])\n",
      "Output data  tensor([[[-0.2428,  0.2256,  0.1109, -0.0539,  0.3551,  0.1985],\n",
      "         [-0.1200,  0.1231,  0.0985, -0.0327,  0.2840,  0.1089],\n",
      "         [-0.2068,  0.2289,  0.0662, -0.0584,  0.3875,  0.1665]],\n",
      "\n",
      "        [[-0.2002,  0.2058,  0.1277, -0.0006,  0.3472,  0.2213],\n",
      "         [-0.1778,  0.2566,  0.1017, -0.0911,  0.3864,  0.1499],\n",
      "         [-0.2514,  0.2692,  0.1715, -0.0407,  0.3557,  0.2312]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Look into these for details\n",
    "%run \"02. MultiHeadAttention.ipynb\"\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"04. FeedForward.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm\n",
    "FeedForward = FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3953f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if verbose: print(f\"\\n=== Transformer Initialization ===\")\n",
    "        \n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.num_heads = cfg[\"n_heads\"]\n",
    "        self.dropout_rate = cfg[\"drop_rate\"]\n",
    "        self.qkv_bias = cfg[\"qkv_bias\"]\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            input_dim = self.embbed_dim,\n",
    "            output_dim = self.embbed_dim,\n",
    "            context_length = self.context_length,\n",
    "            dropout = self.dropout_rate,\n",
    "            num_heads = self.num_heads,\n",
    "            qkv_bias = self.qkv_bias,\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(self.embbed_dim, verbose=verbose)\n",
    "        self.norm1 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.norm2 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dropout rate: \", self.dropout_rate) \n",
    "            print(f\"\\n=== End Transformer Initialization ===\")\n",
    "        \n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "\n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== TransformerBlock Forward Pass ===\")\n",
    "            print(f\"Input shape: {x.shape} (batch_size={batch_size}, context_length={context_length}, input_dim={input_dim})\")\n",
    "            print(f\"Config: num_heads={self.num_heads}, embbed_dim={self.embbed_dim}\")\n",
    "            print(f\"\\nInput tensor (=shortcut) (batch 0 with shape {x[0].shape}):\")\n",
    "            print(f\"States for first batch ...\\n {x[0]}\")\n",
    "            \n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        if verbose: print(f\"\\n1. Normalization 1:\\n {x[0]}\")\n",
    " \n",
    "        x = self.att(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n2. Attention:\\n {x[0]}\")\n",
    " \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n3. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        if verbose: print(f\"\\n4. Output + Shortcut (= new Shortcut):\\n {x[0]}\")\n",
    " \n",
    "        x = self.norm2(x)\n",
    "        if verbose: print(f\"\\n5. Normalization 2:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.ffn(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n6. FeedForward:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n7. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        if verbose: \n",
    "            print(f\"\\n8. Output + new Shortcut:\\n {x[0]}\")\n",
    "            print(f\"\\n===END TransformerBlock Forward Pass ===\\n\")\n",
    "        \n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377056e1",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ade3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([1722, 1528, 2900,  656]) As days turned into\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([1528, 2900,  656, 2745])  days turned into weeks\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.6743,  0.2851,  0.5607,  ..., -0.3712,  0.6174, -0.5773],\n",
      "        [ 0.9675,  0.3970,  0.0029,  ...,  0.7599,  0.8818, -0.1195],\n",
      "        [-1.1204,  1.5178, -0.5717,  ...,  0.1642, -0.5433,  0.8593],\n",
      "        [ 0.5127, -0.5098,  0.7294,  ..., -0.8692, -0.6278, -0.3639]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[-0.3834,  1.2171,  0.0605,  ..., -0.9916,  0.5717,  0.5908],\n",
      "        [-0.0440,  0.6047, -0.7804,  ..., -1.8195,  2.0859, -0.1849],\n",
      "        [ 0.0617,  3.1600,  0.1060,  ..., -0.7261,  0.8990,  0.2640],\n",
      "        [ 1.0921, -1.5200, -1.1416,  ..., -0.8093,  1.4341, -2.5053]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Transformer Initialization ===\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 768\n",
      "    num_heads = 12\n",
      "    head_dim = 64\n",
      "    Generating nn.Linear(768, 768) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(1024, 1024), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.1) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(768, 768) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  768\n",
      "    Hidden dimension =  3072\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "Dropout rate:  0.1\n",
      "\n",
      "=== End Transformer Initialization ===\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([4, 768])):\n",
      "States for first batch ...\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[-0.7232,  1.0827,  0.4612,  ..., -0.9385,  0.8618,  0.0325],\n",
      "        [ 0.6569,  0.7145, -0.5964,  ..., -0.8043,  2.1631, -0.2479],\n",
      "        [-0.6795,  3.2676, -0.2715,  ..., -0.3377,  0.2937,  0.8218],\n",
      "        [ 1.1617, -1.5053, -0.3184,  ..., -1.2475,  0.5757, -2.1212]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.7232,  1.0827,  0.4612,  ..., -0.9385,  0.8618,  0.0325],\n",
      "        [ 0.6569,  0.7145, -0.5964,  ..., -0.8043,  2.1631, -0.2479],\n",
      "        [-0.6795,  3.2676, -0.2715,  ..., -0.3377,  0.2937,  0.8218],\n",
      "        [ 1.1617, -1.5053, -0.3184,  ..., -1.2475,  0.5757, -2.1212]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 12, 4, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 12, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[-1.7295, -1.0389, -1.2549, -1.3818],\n",
      "        [ 4.5911,  3.3291,  4.0017, -4.5853],\n",
      "        [ 2.8569, -1.3261,  0.6317,  2.6327],\n",
      "        [ 1.7609, -3.1073, -2.1644,  2.3187]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[-1.7295,    -inf,    -inf,    -inf],\n",
      "        [ 4.5911,  3.3291,    -inf,    -inf],\n",
      "        [ 2.8569, -1.3261,  0.6317,    -inf],\n",
      "        [ 1.7609, -3.1073, -2.1644,  2.3187]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 12, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5394, 0.4606, 0.0000, 0.0000],\n",
      "        [0.4255, 0.2523, 0.3222, 0.0000],\n",
      "        [0.3097, 0.1685, 0.1896, 0.3321]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1.], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5993, 0.5118, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2803, 0.3580, 0.0000],\n",
      "        [0.3441, 0.0000, 0.2107, 0.3690]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([-1.0316, -0.6136, -1.4830,  ..., -0.5121,  0.9916, -0.8059],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[-1.0316, -0.6136, -1.4830,  ...,  0.9276, -0.4889, -0.3153],\n",
      "        [-0.6206, -0.2598, -0.9994,  ...,  1.1157, -0.3024, -0.1652],\n",
      "        [ 0.2352,  0.1596, -0.1356,  ...,  0.5881, -0.0370,  0.0047],\n",
      "        [-0.1300, -0.5903, -0.1486,  ..., -0.2248, -0.3170,  0.1796]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 768])\n",
      "   Final context vector (batch 0): tensor([[ 0.3049, -0.6248,  0.1046,  ..., -0.1140, -0.2171,  0.4871],\n",
      "        [ 0.3453, -0.2101, -0.2001,  ..., -0.1388,  0.1009,  0.1301],\n",
      "        [ 0.1195, -0.0397,  0.0959,  ..., -0.1680, -0.1563, -0.0461],\n",
      "        [ 0.2701, -0.4378, -0.0254,  ...,  0.0310,  0.0593, -0.0522]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[ 0.3049, -0.6248,  0.1046,  ..., -0.1140, -0.2171,  0.4871],\n",
      "        [ 0.3453, -0.2101, -0.2001,  ..., -0.1388,  0.1009,  0.1301],\n",
      "        [ 0.1195, -0.0397,  0.0959,  ..., -0.1680, -0.1563, -0.0461],\n",
      "        [ 0.2701, -0.4378, -0.0254,  ...,  0.0310,  0.0593, -0.0522]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[ 0.3388, -0.6942,  0.1162,  ..., -0.1267, -0.2412,  0.5412],\n",
      "        [ 0.3837, -0.2335, -0.2223,  ..., -0.1542,  0.1122,  0.1446],\n",
      "        [ 0.1328, -0.0441,  0.1065,  ..., -0.1867, -0.1737, -0.0513],\n",
      "        [ 0.3002, -0.4865, -0.0282,  ...,  0.0345,  0.0659, -0.0580]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[-0.7189,  0.8080,  0.7374,  ..., -1.4895,  0.9479,  0.5547],\n",
      "        [ 1.3072,  0.7682, -0.9998,  ..., -1.2138,  3.0799, -0.1598],\n",
      "        [-0.9259,  4.6336, -0.3592,  ..., -0.7486,  0.1820,  1.0720],\n",
      "        [ 1.9050, -2.5162, -0.4404,  ..., -1.6440,  0.8721, -2.9273]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[-0.4733,  0.5634,  0.5154,  ..., -0.9965,  0.6583,  0.3914],\n",
      "        [ 0.9130,  0.5252, -0.7469,  ..., -0.9008,  2.1885, -0.1425],\n",
      "        [-0.5862,  3.2303, -0.1972,  ..., -0.4645,  0.1744,  0.7853],\n",
      "        [ 1.3498, -1.8362, -0.3404,  ..., -1.2077,  0.6055, -2.1324]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[ 0.1085,  0.0060,  0.1963,  ...,  0.0160, -0.3011,  0.1167],\n",
      "        [-0.1118,  0.1984, -0.1401,  ..., -0.0773, -0.2713,  0.0812],\n",
      "        [ 0.4146,  0.2755,  0.1190,  ..., -0.1835, -0.1659, -0.2940],\n",
      "        [ 0.2001,  0.0213, -0.2903,  ...,  0.0718,  0.1539,  0.1265]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[ 0.1206,  0.0000,  0.0000,  ...,  0.0178, -0.3346,  0.1296],\n",
      "        [-0.0000,  0.2204, -0.1556,  ..., -0.0859, -0.3014,  0.0902],\n",
      "        [ 0.4607,  0.0000,  0.0000,  ..., -0.2039, -0.1844, -0.3267],\n",
      "        [ 0.2223,  0.0236, -0.3225,  ...,  0.0798,  0.1710,  0.1405]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[-5.9831e-01,  8.0800e-01,  7.3739e-01,  ..., -1.4718e+00,\n",
      "          6.1328e-01,  6.8430e-01],\n",
      "        [ 1.3072e+00,  9.8861e-01, -1.1555e+00,  ..., -1.2997e+00,\n",
      "          2.7785e+00, -6.9661e-02],\n",
      "        [-4.6520e-01,  4.6336e+00, -3.5919e-01,  ..., -9.5245e-01,\n",
      "         -2.3208e-03,  7.4534e-01],\n",
      "        [ 2.1273e+00, -2.4926e+00, -7.6293e-01,  ..., -1.5642e+00,\n",
      "          1.0431e+00, -2.7867e+00]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "Output shape:  torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "def use_transformer_block(verbose = False):\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    %run \"01. DataPreparation.ipynb\"\n",
    "    input = get_test_input_embedding(verbose=verbose)\n",
    "\n",
    "    block = TransformerBlock(cfg = GPT_CONFIG_124M, verbose=verbose)\n",
    "    y = block(input, verbose=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Output shape: \", y.shape)\n",
    "\n",
    "if __name__ == \"__main__\": _test_run = use_transformer_block(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

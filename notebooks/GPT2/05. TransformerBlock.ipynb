{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Central Component of GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225ec960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Look into these for details\n",
    "%run \"02. MultiHeadAttention.ipynb\"\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"04. FeedForward.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm\n",
    "FeedForward = FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3953f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if verbose: print(f\"\\n=== Transformer Initialization ===\")\n",
    "        \n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.num_heads = cfg[\"n_heads\"]\n",
    "        self.dropout_rate = cfg[\"drop_rate\"]\n",
    "        self.qkv_bias = cfg[\"qkv_bias\"]\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            input_dim = self.embbed_dim,\n",
    "            output_dim = self.embbed_dim,\n",
    "            context_length = self.context_length,\n",
    "            dropout = self.dropout_rate,\n",
    "            num_heads = self.num_heads,\n",
    "            qkv_bias = self.qkv_bias,\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(self.embbed_dim, verbose=verbose)\n",
    "        self.norm1 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.norm2 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dropout rate: \", self.dropout_rate) \n",
    "            print(f\"\\n=== End Transformer Initialization ===\")\n",
    "        \n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "\n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== TransformerBlock Forward Pass ===\")\n",
    "            print(f\"Input shape: {x.shape} (batch_size={batch_size}, context_length={context_length}, input_dim={input_dim})\")\n",
    "            print(f\"Config: num_heads={self.num_heads}, embbed_dim={self.embbed_dim}\")\n",
    "            print(f\"\\nInput tensor (=shortcut) (batch 0 with shape {x[0].shape}):\")\n",
    "            print(f\"States for first batch ...\\n {x[0]}\")\n",
    "            \n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        if verbose: print(f\"\\n1. Normalization 1:\\n {x[0]}\")\n",
    " \n",
    "        x = self.att(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n2. Attention:\\n {x[0]}\")\n",
    " \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n3. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        if verbose: print(f\"\\n4. Output + Shortcut (= new Shortcut):\\n {x[0]}\")\n",
    " \n",
    "        x = self.norm2(x)\n",
    "        if verbose: print(f\"\\n5. Normalization 2:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.ffn(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n6. FeedForward:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n7. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        if verbose: \n",
    "            print(f\"\\n8. Output + new Shortcut:\\n {x[0]}\")\n",
    "            print(f\"\\n===END TransformerBlock Forward Pass ===\\n\")\n",
    "        \n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377056e1",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ade3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Transformer Initialization ===\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 768\n",
      "    num_heads = 12\n",
      "    head_dim = 64\n",
      "    Generating nn.Linear(768, 768) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(1024, 1024), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.1) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(768, 768) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  768\n",
      "    Hidden dimension =  3072\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "Dropout rate:  0.1\n",
      "\n",
      "=== End Transformer Initialization ===\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([4, 768])):\n",
      "States for first batch ...\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[-0.1554, -1.4885,  1.1975,  ...,  1.4838, -1.3720,  0.6351],\n",
      "        [-2.2864, -0.5368, -0.5850,  ..., -1.5195, -0.3160,  0.8553],\n",
      "        [-0.5828, -1.3211, -1.0055,  ...,  1.0229, -0.0117,  1.4315],\n",
      "        [ 0.1354,  0.1240, -0.2650,  ...,  1.7749, -0.1572,  0.2964]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.1554, -1.4885,  1.1975,  ...,  1.4838, -1.3720,  0.6351],\n",
      "        [-2.2864, -0.5368, -0.5850,  ..., -1.5195, -0.3160,  0.8553],\n",
      "        [-0.5828, -1.3211, -1.0055,  ...,  1.0229, -0.0117,  1.4315],\n",
      "        [ 0.1354,  0.1240, -0.2650,  ...,  1.7749, -0.1572,  0.2964]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 12, 4, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 12, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 5.9173,  2.2190, -2.5383, -1.0682],\n",
      "        [ 1.6525,  2.4131, -0.0236,  4.4639],\n",
      "        [-0.8495, -0.4249,  3.1302,  1.2707],\n",
      "        [ 6.7474, -0.3419,  1.6015, -1.8065]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 5.9173,    -inf,    -inf,    -inf],\n",
      "        [ 1.6525,  2.4131,    -inf,    -inf],\n",
      "        [-0.8495, -0.4249,  3.1302,    -inf],\n",
      "        [ 6.7474, -0.3419,  1.6015, -1.8065]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 12, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4763, 0.5237, 0.0000, 0.0000],\n",
      "        [0.2703, 0.2851, 0.4446, 0.0000],\n",
      "        [0.4384, 0.1807, 0.2304, 0.1505]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5292, 0.5819, 0.0000, 0.0000],\n",
      "        [0.3004, 0.0000, 0.4940, 0.0000],\n",
      "        [0.4871, 0.2008, 0.2560, 0.1672]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([-0.7285,  0.6055,  0.5407,  ..., -0.3041,  0.4352,  0.2047],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[-0.7285,  0.6055,  0.5407,  ...,  0.7011,  0.5506,  1.2655],\n",
      "        [-0.3668,  0.4573, -0.2927,  ...,  0.4582,  0.3691,  0.8678],\n",
      "        [ 0.1416, -0.1479,  0.2244,  ...,  0.2397, -0.0356,  0.6006],\n",
      "        [-0.0542,  0.1312,  0.0183,  ...,  0.4615,  0.1795,  0.6116]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 768])\n",
      "   Final context vector (batch 0): tensor([[ 0.2814,  0.3157, -0.2198,  ..., -0.2446,  0.0405, -0.0445],\n",
      "        [ 0.1559,  0.0256, -0.2838,  ..., -0.5078, -0.0906,  0.2196],\n",
      "        [-0.1220, -0.1169, -0.1949,  ..., -0.0851,  0.0557,  0.3200],\n",
      "        [-0.1787,  0.1156, -0.3130,  ..., -0.3219,  0.0969,  0.1879]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[ 0.2814,  0.3157, -0.2198,  ..., -0.2446,  0.0405, -0.0445],\n",
      "        [ 0.1559,  0.0256, -0.2838,  ..., -0.5078, -0.0906,  0.2196],\n",
      "        [-0.1220, -0.1169, -0.1949,  ..., -0.0851,  0.0557,  0.3200],\n",
      "        [-0.1787,  0.1156, -0.3130,  ..., -0.3219,  0.0969,  0.1879]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[ 0.0000,  0.3508, -0.2443,  ..., -0.2718,  0.0450, -0.0495],\n",
      "        [ 0.1732,  0.0285, -0.3153,  ..., -0.5643, -0.1007,  0.2440],\n",
      "        [-0.1355, -0.1298, -0.2165,  ..., -0.0945,  0.0618,  0.3556],\n",
      "        [-0.1985,  0.1284, -0.3478,  ..., -0.3577,  0.1077,  0.2088]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[-0.3100, -1.8288,  1.3431,  ...,  1.7171, -1.9712,  0.7491],\n",
      "        [-3.1463, -0.7467, -1.1607,  ..., -2.7685, -0.5548,  1.4932],\n",
      "        [-0.8803, -1.8649, -1.5282,  ...,  1.3145,  0.0831,  2.3126],\n",
      "        [-0.0227,  0.2887, -0.7194,  ...,  2.0596, -0.1165,  0.6047]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[-0.1408, -1.1830,  0.9935,  ...,  1.2501, -1.2807,  0.5859],\n",
      "        [-2.1515, -0.5144, -0.7968,  ..., -1.8938, -0.3835,  1.0137],\n",
      "        [-0.6743, -1.3957, -1.1490,  ...,  0.9336,  0.0315,  1.6648],\n",
      "        [-0.0143,  0.2117, -0.5201,  ...,  1.4974, -0.0824,  0.4412]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[-0.2427, -0.0712,  0.0588,  ..., -0.0942, -0.0479,  0.3223],\n",
      "        [-0.3213,  0.1559,  0.1941,  ..., -0.1694,  0.0281, -0.1347],\n",
      "        [ 0.0557,  0.1569, -0.1024,  ...,  0.4714, -0.0099, -0.2526],\n",
      "        [-0.2444,  0.0506, -0.3424,  ..., -0.0046,  0.0467,  0.3079]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[-0.2696, -0.0791,  0.0653,  ..., -0.1046, -0.0532,  0.3581],\n",
      "        [-0.3569,  0.1732,  0.2157,  ..., -0.1882,  0.0312, -0.1497],\n",
      "        [ 0.0619,  0.1743, -0.1138,  ...,  0.5238, -0.0110, -0.2807],\n",
      "        [-0.2716,  0.0562, -0.3805,  ..., -0.0051,  0.0518,  0.3421]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[-0.5796, -1.9079,  1.4084,  ...,  1.6124, -2.0244,  1.1072],\n",
      "        [-3.5032, -0.5735, -0.9450,  ..., -2.9567, -0.5236,  1.3435],\n",
      "        [-0.8184, -1.6906, -1.6420,  ...,  1.8383,  0.0722,  2.0319],\n",
      "        [-0.2942,  0.3449, -1.0998,  ...,  2.0545, -0.0646,  0.9469]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "Output shape:  torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "def use_transformer_block(verbose = False):\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    %run \"01. DataPreparation.ipynb\"\n",
    "    input = get_test_input_embedding(verbose=verbose)\n",
    "\n",
    "    block = TransformerBlock(cfg = GPT_CONFIG_124M, verbose=verbose)\n",
    "    y = block(input, verbose=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Output shape: \", y.shape)\n",
    "\n",
    "if '__file__' not in dir(): _test_run = use_transformer_block(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

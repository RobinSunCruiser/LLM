{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Central Component of GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225ec960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Look into these for details\n",
    "%run \"02. MultiHeadAttention.ipynb\"\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"04. FeedForward.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm\n",
    "FeedForward = FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8b3f1",
   "metadata": {},
   "source": [
    "## Transformer block implementation\n",
    "A single Transformer layer that processes token embeddings through two main operations with residual connections:\n",
    "1. Self-Attention path: LayerNorm → Multi-Head Attention → Dropout → Add residual\n",
    "2. Feed-Forward path: LayerNorm → Feed-Forward Network → Dropout → Add residual\n",
    "\n",
    "Each path uses Pre-LN architecture (normalize before the operation) and residual connections (shortcuts) to stabilize training and enable gradient flow through deep networks. The block transforms input embeddings while preserving their dimensionality, allowing tokens to exchange information (attention) and learn complex representations (FF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3953f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if verbose: print(f\"\\n=== Transformer Initialization ===\")\n",
    "        \n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.num_heads = cfg[\"n_heads\"]\n",
    "        self.dropout_rate = cfg[\"drop_rate\"]\n",
    "        self.qkv_bias = cfg[\"qkv_bias\"]\n",
    "\n",
    "        self.att = MultiHeadAttention(\n",
    "            input_dim = self.embbed_dim,\n",
    "            output_dim = self.embbed_dim,\n",
    "            context_length = self.context_length,\n",
    "            dropout = self.dropout_rate,\n",
    "            num_heads = self.num_heads,\n",
    "            qkv_bias = self.qkv_bias,\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(self.embbed_dim, verbose=verbose)\n",
    "        self.norm1 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.norm2 = LayerNorm(self.embbed_dim, verbose=verbose)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Dropout rate: \", self.dropout_rate) \n",
    "            print(f\"\\n=== End Transformer Initialization ===\")\n",
    "        \n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "\n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== TransformerBlock Forward Pass ===\")\n",
    "            print(f\"Input shape: {x.shape} (batch_size={batch_size}, context_length={context_length}, input_dim={input_dim})\")\n",
    "            print(f\"Config: num_heads={self.num_heads}, embbed_dim={self.embbed_dim}\")\n",
    "            print(f\"\\nInput tensor (=shortcut) (batch 0 with shape {x[0].shape}):\")\n",
    "            print(f\"States for first batch ...\\n {x[0]}\")\n",
    "            \n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        if verbose: print(f\"\\n1. Normalization 1:\\n {x[0]}\")\n",
    " \n",
    "        x = self.att(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n2. Attention:\\n {x[0]}\")\n",
    " \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n3. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        if verbose: print(f\"\\n4. Output + Shortcut (= new Shortcut):\\n {x[0]}\")\n",
    " \n",
    "        x = self.norm2(x)\n",
    "        if verbose: print(f\"\\n5. Normalization 2:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.ff(x, verbose = verbose)\n",
    "        if verbose: print(f\"\\n6. FeedForward:\\n {x[0]}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        if verbose: print(f\"\\n7. Dropout:\\n {x[0]}\")\n",
    "        \n",
    "        x = x + shortcut\n",
    "        if verbose: \n",
    "            print(f\"\\n8. Output + new Shortcut:\\n {x[0]}\")\n",
    "            print(f\"\\n===END TransformerBlock Forward Pass ===\\n\")\n",
    "        \n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377056e1",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ade3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([ 632, 3160,  287,  262])  <-->   It lives in the\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([ 3160,   287,   262, 25152])  <-->   lives in the mug\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 token x 768 dimensions):\n",
      " tensor([[-1.5095, -2.4683, -0.4507,  ...,  0.1136, -0.4039, -0.1103],\n",
      "        [ 0.2111, -0.8547, -2.6263,  ...,  0.0209,  0.3960,  0.2114],\n",
      "        [-0.6407,  0.5064,  0.6585,  ..., -1.0166,  0.2024,  1.1962],\n",
      "        [ 0.7657, -0.7868, -0.6283,  ..., -1.0856, -0.4774, -0.8335]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 positions x 768 dimensions):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.8485, -3.8955,  2.0098,  ...,  1.3554, -1.5149,  0.9645],\n",
      "        [-1.1852, -0.9347, -1.5547,  ..., -0.6137,  0.4853,  0.8941],\n",
      "        [-0.8895, -0.1195, -0.1786,  ..., -0.4397,  0.1648,  1.6310],\n",
      "        [ 1.6885, -0.2066, -0.9251,  ...,  0.2178, -0.9156, -0.2817]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Transformer Initialization ===\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 768\n",
      "    num_heads = 12\n",
      "    head_dim = 64\n",
      "    Generating nn.Linear(768, 768) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(1024, 1024), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.1) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(768, 768) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  768\n",
      "    Hidden dimension =  3072\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "Dropout rate:  0.1\n",
      "\n",
      "=== End Transformer Initialization ===\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([4, 768])):\n",
      "States for first batch ...\n",
      " tensor([[-0.8485, -3.8955,  2.0098,  ...,  1.3554, -1.5149,  0.9645],\n",
      "        [-1.1852, -0.9347, -1.5547,  ..., -0.6137,  0.4853,  0.8941],\n",
      "        [-0.8895, -0.1195, -0.1786,  ..., -0.4397,  0.1648,  1.6310],\n",
      "        [ 1.6885, -0.2066, -0.9251,  ...,  0.2178, -0.9156, -0.2817]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[-0.6012, -2.7241,  1.3902,  ...,  0.9342, -1.0655,  0.6619],\n",
      "        [-0.8657, -0.6880, -1.1277,  ..., -0.4605,  0.3188,  0.6088],\n",
      "        [-0.5989, -0.0704, -0.1109,  ..., -0.2901,  0.1248,  1.1313],\n",
      "        [ 1.1688, -0.1680, -0.6748,  ...,  0.1314, -0.6681, -0.2210]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.6012, -2.7241,  1.3902,  ...,  0.9342, -1.0655,  0.6619],\n",
      "        [-0.8657, -0.6880, -1.1277,  ..., -0.4605,  0.3188,  0.6088],\n",
      "        [-0.5989, -0.0704, -0.1109,  ..., -0.2901,  0.1248,  1.1313],\n",
      "        [ 1.1688, -0.1680, -0.6748,  ...,  0.1314, -0.6681, -0.2210]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 12, 4, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 12, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 3.5332,  2.6669, -1.4881, -0.9143],\n",
      "        [ 2.1825, -0.0881, -0.3918, -3.8974],\n",
      "        [-4.4848,  3.2532, -1.8340,  2.5347],\n",
      "        [ 4.0659,  1.1936,  1.5413,  1.1984]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 3.5332,    -inf,    -inf,    -inf],\n",
      "        [ 2.1825, -0.0881,    -inf,    -inf],\n",
      "        [-4.4848,  3.2532, -1.8340,    -inf],\n",
      "        [ 4.0659,  1.1936,  1.5413,  1.1984]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 12, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5705, 0.4295, 0.0000, 0.0000],\n",
      "        [0.1991, 0.5237, 0.2773, 0.0000],\n",
      "        [0.3198, 0.2234, 0.2333, 0.2235]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6339, 0.4772, 0.0000, 0.0000],\n",
      "        [0.2212, 0.0000, 0.3081, 0.0000],\n",
      "        [0.3554, 0.2482, 0.2592, 0.2483]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([-0.4651,  0.9243, -0.0982,  ..., -0.2498,  0.4174,  0.8493],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[-0.4651,  0.9243, -0.0982,  ...,  1.1714,  0.0017,  0.6545],\n",
      "        [-0.6137,  0.9963, -0.5874,  ...,  0.9067, -0.2682,  0.5309],\n",
      "        [ 0.1693,  0.2142, -0.1195,  ...,  0.8011, -0.4440,  0.4910],\n",
      "        [ 0.2058,  0.7535, -0.5940,  ...,  0.8060, -0.2347,  0.2509]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 768])\n",
      "   Final context vector (batch 0): tensor([[ 0.4061, -0.5669, -0.4598,  ..., -0.6580, -0.1312, -0.9290],\n",
      "        [-0.0222, -0.6821, -0.4549,  ..., -0.3929, -0.2183, -0.3468],\n",
      "        [ 0.1505, -0.2917, -0.0842,  ..., -0.1834, -0.4257, -0.2595],\n",
      "        [ 0.0216, -0.4344, -0.1581,  ..., -0.3504, -0.1479, -0.0591]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[ 0.4061, -0.5669, -0.4598,  ..., -0.6580, -0.1312, -0.9290],\n",
      "        [-0.0222, -0.6821, -0.4549,  ..., -0.3929, -0.2183, -0.3468],\n",
      "        [ 0.1505, -0.2917, -0.0842,  ..., -0.1834, -0.4257, -0.2595],\n",
      "        [ 0.0216, -0.4344, -0.1581,  ..., -0.3504, -0.1479, -0.0591]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[ 0.0000, -0.6298, -0.5109,  ..., -0.7312, -0.1457, -1.0322],\n",
      "        [-0.0247, -0.7579, -0.5055,  ..., -0.4366, -0.2426, -0.3853],\n",
      "        [ 0.1672, -0.3241, -0.0936,  ..., -0.2038, -0.4730, -0.2883],\n",
      "        [ 0.0240, -0.4826, -0.1756,  ..., -0.3893, -0.1643, -0.0656]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[-0.8485, -4.5254,  1.4990,  ...,  0.6242, -1.6607, -0.0677],\n",
      "        [-1.2099, -1.6926, -2.0602,  ..., -1.0503,  0.2427,  0.5088],\n",
      "        [-0.7223, -0.4436, -0.2721,  ..., -0.6435, -0.3082,  1.3426],\n",
      "        [ 1.7125, -0.6892, -1.1007,  ..., -0.1715, -1.0799, -0.3474]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[-0.5791, -3.0438,  0.9945,  ...,  0.4081, -1.1235, -0.0557],\n",
      "        [-0.8637, -1.1972, -1.4512,  ..., -0.7535,  0.1399,  0.3237],\n",
      "        [-0.4739, -0.2856, -0.1698,  ..., -0.4207, -0.1942,  0.9213],\n",
      "        [ 1.1649, -0.5137, -0.8013,  ..., -0.1518, -0.7867, -0.2748]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[-0.1851, -0.0855,  0.0438,  ..., -0.0974, -0.2166, -0.0568],\n",
      "        [-0.2866, -0.0384,  0.1446,  ...,  0.1479, -0.0536, -0.1259],\n",
      "        [-0.0023, -0.1745,  0.2108,  ...,  0.2784, -0.5047, -0.2318],\n",
      "        [-0.2712,  0.0333, -0.0257,  ..., -0.0755, -0.1114,  0.4207]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[-0.2057, -0.0950,  0.0486,  ..., -0.1082, -0.2406, -0.0632],\n",
      "        [-0.3184, -0.0427,  0.1607,  ...,  0.1643, -0.0595, -0.1398],\n",
      "        [-0.0025, -0.1939,  0.2343,  ...,  0.3093, -0.5608, -0.2575],\n",
      "        [-0.3013,  0.0370, -0.0286,  ..., -0.0839, -0.1238,  0.4675]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[-1.0542, -4.6204,  1.5476,  ...,  0.5161, -1.9013, -0.1309],\n",
      "        [-1.5283, -1.7353, -1.8995,  ..., -0.8860,  0.1832,  0.3689],\n",
      "        [-0.7248, -0.6375, -0.0379,  ..., -0.3342, -0.8690,  1.0851],\n",
      "        [ 1.4112, -0.6522, -1.1293,  ..., -0.2553, -1.2037,  0.1201]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "Output shape:  torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "def use_transformer_block(verbose = False):\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    %run \"01. DataPreparation.ipynb\"\n",
    "    input = get_test_input_embedding(verbose=verbose)\n",
    "\n",
    "    block = TransformerBlock(cfg = GPT_CONFIG_124M, verbose=verbose)\n",
    "    y = block(input, verbose=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Output shape: \", y.shape)\n",
    "\n",
    "if '__file__' not in dir(): _test_run = use_transformer_block(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Prototype Attention Head\n",
    "Implementation of a single attention head (Just for demonstration of attention principle. Not used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1453b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## This is just for demonstration purposes. Not used later\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    # input embedding dimension must not be same as output embedding dimension\n",
    "    # context length sets number of tokens in input \n",
    "    # dropout rate is a value for randomly setting weights to 0 (prevents overfitting)\n",
    "    # qkv_bias ads learnable bias weights to nn.Linear. Not used in GPT2 no significant improvement\n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # init weight matrices that are later used for input -> query, key, value projection\n",
    "        # all projections (every token) are done with these same matrices\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        \n",
    "        # init dropout (prevent overfitting)\n",
    "        self.dropout = nn.Dropout(dropout) # dropout is a probability\n",
    "        \n",
    "        # registers static causal masking matrix (diagonal) in same device as model (is self.mask)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # Causal Masking on diagonal triangle\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # stores dimensions of input (here only context length needed)\n",
    "        batch_size, con_len, in_dim = x.shape # set them to the shape of x ( 8, 4, 256 )\n",
    "        \n",
    "        # using the weight matrices for projection of the input to query, key and value\n",
    "        # broadcasting (for badges)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # calculates all attention scores\n",
    "        # Compares query for every input token to all keys\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # creates bool mask (diagonal) fills all causal invisible elements with -inf\n",
    "        attn_scores.masked_fill( self.mask.bool()[:con_len, :con_len], -torch.inf)\n",
    "\n",
    "        # calculations of soft max scaled with sqrt of keys length dimension (keys.shape)\n",
    "        # note: dim=-1 takes most internal dimension in an array \n",
    "        # example: [ [ [ 1, 2, 3] ],  [ [ 4, 5, 6 ], [ 7, 8, 8 ] ] ] -> 3\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # applying dropout to weights\n",
    "        # randomly sets weights to 0 based on probability\n",
    "        # !! scales remaining values up to compensate dropped weights\n",
    "        attn_weights = self.dropout(attn_weights) # Additional random dropout\n",
    "\n",
    "        # sum of attention weighted values for every input (including causal masked knowledge)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f117a2",
   "metadata": {},
   "source": [
    "## Short insight in masking and causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4820aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example matrix with random attention_scores:\n",
      " tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936],\n",
      "        [0.8694, 0.5677, 0.7411, 0.4294]])\n",
      "\n",
      "Buffered causal (autoregressive) mask:\n",
      " tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "mask.bool():\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "mask.bool() clipped to match input dimension (context):\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "matrix.masked_fill(mask.bool(), -inf):\n",
      " tensor([[0.8823,   -inf,   -inf,   -inf],\n",
      "        [0.3904, 0.6009,   -inf,   -inf],\n",
      "        [0.9408, 0.1332, 0.9346,   -inf],\n",
      "        [0.8694, 0.5677, 0.7411, 0.4294]])\n"
     ]
    }
   ],
   "source": [
    "# Triangular Matrix filtering example for causal masking\n",
    "def mask_demo():\n",
    "\n",
    "    # matrix to mask. Example attention scores\n",
    "    matrix = torch.rand(4,4)\n",
    "\n",
    "    # buffered static causal mask\n",
    "    mask = torch.triu(torch.ones(4, 4), diagonal=1)\n",
    "\n",
    "    print(\"\\nExample matrix with random attention_scores:\\n\", matrix)\n",
    "    print(\"\\nBuffered causal (autoregressive) mask:\\n\", mask)  # Triangle mask: 0s below diagonal, 1s above\n",
    "    print(\"\\nmask.bool():\\n\", mask.bool())  # Converts to boolean: 0→False, 1→True\n",
    "    print(\"\\nmask.bool() clipped to match input dimension (context):\\n\", mask.bool()[:3, :3])  # 3x3 sub-matrix\n",
    "    print(\"\\nmatrix.masked_fill(mask.bool(), -inf):\\n\", matrix.masked_fill(mask.bool()[:4, :4], -torch.inf))  # Sets upper triangle to -inf\n",
    "\n",
    "if '__file__' not in dir(): mask_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a790",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "An implementation of Multi Head Attention.\n",
    "Multiple heads train themselves on different parts of the input embedding. The input is projected to Q, K, V matrices (all at once), then split into \"head\"-sized slices.\n",
    "Each head computes attention scores (distance between Q and K) (Q @ K^T) and attention weights (softmax) independently on its slice.\n",
    "The resulting context vectors from all heads are concatenated and projected through a final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4c94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, num_heads, qkv_bias=False, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check if output dimension is dividable by attention head number without rest\n",
    "        # Input is split to num_head chunks of length head_dim\n",
    "        assert output_dim % num_heads == 0, \"Output dimension must be dividable by head_num\"\n",
    "\n",
    "        self.TransformerBlock = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = output_dim // num_heads\n",
    "\n",
    "        # Init weight matrices for input to qkv projection (full not yet separated into head parts)\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.out_proj = nn.Linear(output_dim, output_dim) # Linear layer for combination of head outputs\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n=== MultiHeadAttention Initialization ===\")\n",
    "            print(f\"    input_dim =\", input_dim)\n",
    "            print(f\"    output_dim =\", output_dim)\n",
    "            print(f\"    num_heads =\", self.num_heads)\n",
    "            print(f\"    head_dim =\", self.head_dim)\n",
    "            print(f\"    Generating nn.Linear({input_dim}, {output_dim}) weights for query, key and value\")\n",
    "            print(f\"    Generating causal diagonal mask torch.triu(torch.ones({context_length}, {context_length}), diagonal=1) for causal masking of attn_scores\")        \n",
    "            print(f\"    Generating dropout nn.Dropout({dropout}) for random dropout of attn_weights\")        \n",
    "            print(f\"    Generating optional nn.Linear({output_dim}, {output_dim}) weights for final context_vector projection\")\n",
    "            print(f\"=== End MultiHeadAttention Initialization ===\\n\")\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "\n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== MultiHeadAttention Forward Pass ===\")\n",
    "            print(f\"Input shape: {x.shape} (batch_size={batch_size}, context_length={context_length}, input_dim={input_dim})\")\n",
    "            print(f\"Config: num_heads={self.num_heads}, head_dim={self.head_dim}, output_dim={self.num_heads * self.head_dim}\")\n",
    "            print(f\"\\nInput tensor (batch 0 with shape {x[0].shape}):\")\n",
    "            print(f\" {x[0]}\")\n",
    "\n",
    "        # using weight matrices for projection of the input to qkv (not yet splitted for attention heads)\n",
    "        # broadcasting for badges\n",
    "        # -> shape: batch_size, context_length, output_dim\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n1. QKV Projection:\")\n",
    "            print(f\"   QKV shapes: {queries.shape}\")\n",
    " \n",
    "        # Implicitly splitting matrix by adding head_num dimension\n",
    "        # Unrol last dim: (batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
    "        # Example ( 8, 4, 256 ) -> ( 8, 4, 8, 32 ) for num_heads = 8 and head_dim = 32\n",
    "        queries = queries.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n2. Split into heads:\")\n",
    "            print(f\"(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\")\n",
    "            print(f\"   QKV shapes after view: {queries.shape}\")\n",
    " \n",
    "        # Transpose to use for query comparison - move num_head to front\n",
    "        # (batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n3. Transpose for attention computation:\")\n",
    "            print(f\"(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\")\n",
    "            print(f\"   QKV shapes after transpose: {queries.shape}\")\n",
    "\n",
    "        # Compute scaled dot_production attention\n",
    "        attn_scores = queries @ keys.transpose(2,3) # Dot product for each head\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n4. Attention scores computation:\")\n",
    "            print(f\"   attn_scores shape: {attn_scores.shape}\")\n",
    "            print(f\"   Scale factor (1/sqrt(head_dim)): {1/keys.shape[-1]**0.5:.4f}\")\n",
    "            print(f\"   Raw attention scores for head 0, batch 0:\\n{attn_scores[0, 0]}\")\n",
    "\n",
    "        # Causal Masking\n",
    "        mask_bool = self.mask.bool()[:context_length, :context_length]\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n5. Causal masking:\")\n",
    "            print(f\"   Causal mask:\\n{mask_bool}\\n\")\n",
    "            print(f\"   Then masked_fill True -> -torch.inf\\n\")\n",
    "            print(f\"   Masked attention scores for head 0, batch 0:\\n{attn_scores[0, 0]}\")\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim =-1)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n6. Softmax attention weights:\")\n",
    "            print(f\"   attn_weights shape: {attn_weights.shape}\")\n",
    "            print(f\"   Attention weights for head 0, batch 0:\\n{attn_weights[0, 0]}\\n\")\n",
    "            print(f\"   Sum of weights (should be ~1.0): {attn_weights[0, 0].sum(dim=-1)}\")\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n7. After dropout:\")\n",
    "            print(f\"   Attention weights after dropout for head 0, batch 0:\\n{attn_weights[0, 0]}\")\n",
    "\n",
    "        # build context vector switch back num_heads and context_length, combining heads\n",
    "        # self.output_dim = self.num_heads * self.head_dim\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n8. Compute context vectors:\")\n",
    "            print(f\"   context_vec shape after attention: {context_vec.shape}\")\n",
    "            print(f\"   First context vector (batch 0, token 0, head 0): {context_vec[0, 0, 0]}...\")\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(batch_size, context_length, self.num_heads * self.head_dim)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n9. Concatenate heads:\")\n",
    "            print(f\"   context_vec shape after view: {context_vec.shape}\")\n",
    "            print(f\"   First concatenated context vector (batch 0): {context_vec[0]}...\")\n",
    "\n",
    "        context_vec = self.out_proj(context_vec) # optional projection by Linear layer\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n10. Final output projection:\")\n",
    "            print(f\"   Final context_vec shape: {context_vec.shape}\")\n",
    "            print(f\"   Final context vector (batch 0): {context_vec[0]}...\")\n",
    "            print(f\"=== End MultiHeadAttention Forward Pass ===\\n\")\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b733d",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8420924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 128\n",
      "    num_heads = 8\n",
      "    head_dim = 16\n",
      "    Generating nn.Linear(768, 128) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "\n",
      "----- Using real data input (see DataPreparation.ipynb) --\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([ 632, 3160,  287,  262])  <-->   It lives in the\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([ 3160,   287,   262, 25152])  <-->   lives in the mug\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 token x 768 dimensions):\n",
      " tensor([[-1.5095, -2.4683, -0.4507,  ...,  0.1136, -0.4039, -0.1103],\n",
      "        [ 0.2111, -0.8547, -2.6263,  ...,  0.0209,  0.3960,  0.2114],\n",
      "        [-0.6407,  0.5064,  0.6585,  ..., -1.0166,  0.2024,  1.1962],\n",
      "        [ 0.7657, -0.7868, -0.6283,  ..., -1.0856, -0.4774, -0.8335]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 positions x 768 dimensions):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.8485, -3.8955,  2.0098,  ...,  1.3554, -1.5149,  0.9645],\n",
      "        [-1.1852, -0.9347, -1.5547,  ..., -0.6137,  0.4853,  0.8941],\n",
      "        [-0.8895, -0.1195, -0.1786,  ..., -0.4397,  0.1648,  1.6310],\n",
      "        [ 1.6885, -0.2066, -0.9251,  ...,  0.2178, -0.9156, -0.2817]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=8, head_dim=16, output_dim=128\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.8485, -3.8955,  2.0098,  ...,  1.3554, -1.5149,  0.9645],\n",
      "        [-1.1852, -0.9347, -1.5547,  ..., -0.6137,  0.4853,  0.8941],\n",
      "        [-0.8895, -0.1195, -0.1786,  ..., -0.4397,  0.1648,  1.6310],\n",
      "        [ 1.6885, -0.2066, -0.9251,  ...,  0.2178, -0.9156, -0.2817]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 128])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 8, 16])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 8, 4, 16])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 8, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.2500\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 1.8414,  1.9165,  0.3041, -0.2983],\n",
      "        [ 2.3356,  1.1054, -0.7927, -1.2917],\n",
      "        [ 0.6775,  0.6689, -0.0494, -0.5733],\n",
      "        [-0.7529, -2.9828,  0.4309,  0.5816]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 1.8414,    -inf,    -inf,    -inf],\n",
      "        [ 2.3356,  1.1054,    -inf,    -inf],\n",
      "        [ 0.6775,  0.6689, -0.0494,    -inf],\n",
      "        [-0.7529, -2.9828,  0.4309,  0.5816]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 8, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5763, 0.4237, 0.0000, 0.0000],\n",
      "        [0.3531, 0.3524, 0.2945, 0.0000],\n",
      "        [0.2319, 0.1328, 0.3117, 0.3237]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1.], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7204, 0.5296, 0.0000, 0.0000],\n",
      "        [0.4414, 0.4405, 0.3681, 0.0000],\n",
      "        [0.2898, 0.1660, 0.3896, 0.4046]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 8, 16])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([-0.5407, -2.6193, -2.7024,  ..., -1.3786,  0.2218, -1.3166],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 128])\n",
      "   First concatenated context vector (batch 0): tensor([[-0.5407, -2.6193, -2.7024,  ..., -0.2488, -1.6782, -2.6641],\n",
      "        [-0.5971, -1.5779, -1.5019,  ..., -0.3285, -0.8478, -1.6947],\n",
      "        [-0.1880, -0.9441, -1.3256,  ..., -0.3215,  0.2135, -0.4000],\n",
      "        [-0.1775, -0.5374, -0.9472,  ..., -0.3776,  0.0819, -0.0281]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 128])\n",
      "   Final context vector (batch 0): tensor([[ 0.0164,  0.5055, -0.5678,  ...,  0.1771,  0.5125,  0.5688],\n",
      "        [ 0.6812,  0.0431, -0.4950,  ...,  0.7756,  0.1800,  0.8339],\n",
      "        [ 0.3688, -0.0163, -0.7047,  ...,  0.1630,  0.2258,  0.3866],\n",
      "        [ 0.1867,  0.1901, -0.4252,  ...,  0.4955,  0.7117,  0.2625]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def multi_head_attention_test_run(verbose=False):\n",
    "    if verbose: print(\"\\n\\n------- initializing Multi Head Attention ----------------\\n\")\n",
    "    mha = MultiHeadAttention(input_dim=768, output_dim=128, context_length=4, dropout=0.2, num_heads=8, verbose=verbose)\n",
    "\n",
    "    if verbose: print(\"\\n\\n----- Using real data input (see DataPreparation.ipynb) --\\n\")\n",
    "    %run \"./01. DataPreparation.ipynb\"\n",
    "    batch = get_test_input_embedding(verbose=verbose) # defined in DataPreparation.ipynb\n",
    "\n",
    "    if verbose: print(\"\\n\\n------- performing multi head attention ------------------\\n\")\n",
    "    context_vector = mha(batch, verbose=verbose)\n",
    "\n",
    "if '__file__' not in dir(): _test_run = multi_head_attention_test_run(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

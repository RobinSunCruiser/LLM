{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for GPT_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225ec960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embbed_dim:  6\n",
      "Test input\n",
      ": tensor([[0.2483, 0.0000, 0.0000, 0.4067, 0.4628, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5766, 1.7535, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1863],\n",
      "        [0.3884]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0466],\n",
      "        [0.5005]], grad_fn=<VarBackward0>)\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 6\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(6))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(6))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Forward Pass ===\n",
      "    Input (first 5 elements):  tensor([0.2483, 0.0000, 0.0000, 0.4067, 0.4628], grad_fn=<SliceBackward0>)\n",
      "    Normalized (norm_x first 5 elements):  tensor([ 0.3144, -0.9452, -0.9452,  1.1184,  1.4030], grad_fn=<SliceBackward0>)\n",
      "    Output = self.scale * norm_x + self.shift\n",
      "=== End LayerNorm Forward Pass ===\n",
      "\n",
      "\n",
      "matrix (ex. attention_scores):\n",
      " tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936],\n",
      "        [0.8694, 0.5677, 0.7411, 0.4294]])\n",
      "\n",
      "mask (buffered causal_mask of size context_length):\n",
      " tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "mask.bool():\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "mask.bool()[interval] (clipped to match input)\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "matrix.mask_fill(mask.bool()[interval], value)\n",
      " tensor([[0.8823,   -inf,   -inf,   -inf],\n",
      "        [0.3904, 0.6009,   -inf,   -inf],\n",
      "        [0.9408, 0.1332, 0.9346,   -inf],\n",
      "        [0.8694, 0.5677, 0.7411, 0.4294]])\n",
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 128\n",
      "    num_heads = 8\n",
      "    head_dim = 16\n",
      "    Generating nn.Linear(768, 128) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "\n",
      "----- Using real data input (see DataPreparation.ipynb) --\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([1722, 1528, 2900,  656]) As days turned into\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([1528, 2900,  656, 2745])  days turned into weeks\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.6743,  0.2851,  0.5607,  ..., -0.3712,  0.6174, -0.5773],\n",
      "        [ 0.9675,  0.3970,  0.0029,  ...,  0.7599,  0.8818, -0.1195],\n",
      "        [-1.1204,  1.5178, -0.5717,  ...,  0.1642, -0.5433,  0.8593],\n",
      "        [ 0.5127, -0.5098,  0.7294,  ..., -0.8692, -0.6278, -0.3639]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[-0.3834,  1.2171,  0.0605,  ..., -0.9916,  0.5717,  0.5908],\n",
      "        [-0.0440,  0.6047, -0.7804,  ..., -1.8195,  2.0859, -0.1849],\n",
      "        [ 0.0617,  3.1600,  0.1060,  ..., -0.7261,  0.8990,  0.2640],\n",
      "        [ 1.0921, -1.5200, -1.1416,  ..., -0.8093,  1.4341, -2.5053]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=8, head_dim=16, output_dim=128\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 128])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 8, 16])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 8, 4, 16])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 8, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.2500\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[-2.3355, -5.7764,  0.4444,  1.9685],\n",
      "        [ 1.2038,  0.0070,  0.6662,  0.7049],\n",
      "        [ 0.6888,  1.8636, -2.4449, -2.6901],\n",
      "        [-0.1201,  0.8956, -0.5634, -0.6249]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[-2.3355,    -inf,    -inf,    -inf],\n",
      "        [ 1.2038,  0.0070,    -inf,    -inf],\n",
      "        [ 0.6888,  1.8636, -2.4449,    -inf],\n",
      "        [-0.1201,  0.8956, -0.5634, -0.6249]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 8, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5742, 0.4258, 0.0000, 0.0000],\n",
      "        [0.3574, 0.4794, 0.1633, 0.0000],\n",
      "        [0.2460, 0.3171, 0.2202, 0.2168]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7178, 0.5322, 0.0000, 0.0000],\n",
      "        [0.4467, 0.5992, 0.2041, 0.0000],\n",
      "        [0.3075, 0.3963, 0.2752, 0.0000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 8, 16])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 128])\n",
      "   First concatenated context vector (batch 0): tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.5999,  0.0693, -2.0219],\n",
      "        [-0.6335,  0.3727, -0.2214,  ..., -1.1906,  0.3801, -1.1622],\n",
      "        [-0.5339,  0.5235, -0.1759,  ..., -0.5504, -0.1153, -0.5928],\n",
      "        [-0.4721,  0.5994, -0.2717,  ..., -0.7453,  0.2513, -0.9074]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 128])\n",
      "   Final context vector (batch 0): tensor([[ 0.1855, -0.0657,  0.1890,  ...,  0.1948, -0.5213,  0.1647],\n",
      "        [ 0.0250, -0.0465,  0.6121,  ...,  0.2147, -0.4758,  0.0382],\n",
      "        [-0.1077,  0.0907, -0.2580,  ..., -0.0040, -0.3545, -0.0425],\n",
      "        [ 0.0957,  0.2669,  0.1539,  ...,  0.1370, -0.2130,  0.6790]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "Embbed_dim:  6\n",
      "Test input\n",
      ": tensor([[0.2483, 0.0000, 0.0000, 0.4067, 0.4628, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5766, 1.7535, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1863],\n",
      "        [0.3884]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0466],\n",
      "        [0.5005]], grad_fn=<VarBackward0>)\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 6\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(6))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(6))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Forward Pass ===\n",
      "    Input (first 5 elements):  tensor([0.2483, 0.0000, 0.0000, 0.4067, 0.4628], grad_fn=<SliceBackward0>)\n",
      "    Normalized (norm_x first 5 elements):  tensor([ 0.3144, -0.9452, -0.9452,  1.1184,  1.4030], grad_fn=<SliceBackward0>)\n",
      "    Output = self.scale * norm_x + self.shift\n",
      "=== End LayerNorm Forward Pass ===\n",
      "\n",
      "Embbed_dim:  6\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  6\n",
      "    Hidden dimension =  24\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "Sample data:  tensor([[[0.4234, 0.6038, 0.1525, 0.3970, 0.8703, 0.7563],\n",
      "         [0.1836, 0.0991, 0.1583, 0.0066, 0.1142, 0.3764],\n",
      "         [0.8374, 0.5837, 0.1197, 0.0989, 0.7487, 0.1281]],\n",
      "\n",
      "        [[0.4384, 0.7399, 0.2686, 0.4455, 0.4565, 0.3817],\n",
      "         [0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585],\n",
      "         [0.1642, 0.6212, 0.6378, 0.7740, 0.8801, 0.7784]]])\n",
      "\n",
      "Output shape  torch.Size([2, 3, 6])\n",
      "Output data  tensor([[[-0.2428,  0.2256,  0.1109, -0.0539,  0.3551,  0.1985],\n",
      "         [-0.1200,  0.1231,  0.0985, -0.0327,  0.2840,  0.1089],\n",
      "         [-0.2068,  0.2289,  0.0662, -0.0584,  0.3875,  0.1665]],\n",
      "\n",
      "        [[-0.2002,  0.2058,  0.1277, -0.0006,  0.3472,  0.2213],\n",
      "         [-0.1778,  0.2566,  0.1017, -0.0911,  0.3864,  0.1499],\n",
      "         [-0.2514,  0.2692,  0.1715, -0.0407,  0.3557,  0.2312]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([15424,   373,   257,  5909])  archive was a vast\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([  373,   257,  5909, 16099])  was a vast repository\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.9710, -0.7524, -0.8731,  ...,  0.7471, -0.9052, -0.2762],\n",
      "        [-1.9231, -0.6952, -1.9170,  ..., -1.5696, -0.5434,  0.5664],\n",
      "        [-0.4960, -1.1091, -0.4747,  ...,  0.8321,  0.0589,  1.5222],\n",
      "        [-0.7470, -0.4200, -0.0747,  ...,  1.1139,  0.2141, -0.1558]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[ 0.6610, -1.4272,  2.4605,  ...,  1.2418, -1.1110,  1.0747],\n",
      "        [-1.3963, -0.0800,  1.0716,  ..., -0.6346,  0.0893,  0.6827],\n",
      "        [-0.2487, -0.6259, -0.8370,  ...,  0.5769, -0.0376,  0.4348],\n",
      "        [ 0.9228,  0.5802, -0.2968,  ...,  1.3034, -0.4382,  0.5517]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-0.3100, -2.1796,  1.5873,  ...,  1.9889, -2.0162,  0.7986],\n",
      "        [-3.3194, -0.7751, -0.8453,  ..., -2.2042, -0.4541,  1.2492],\n",
      "        [-0.7448, -1.7351, -1.3117,  ...,  1.4090,  0.0213,  1.9570],\n",
      "        [ 0.1758,  0.1602, -0.3716,  ...,  2.4173, -0.2241,  0.3959]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Embedder Initialization ===\n",
      "    vocab_size =  50252\n",
      "    context_length =  4\n",
      "    embedding_dim =  768\n",
      "    Generating token_embeddings (50252 x 768)\n",
      "    Generating pos_embeddings (4 x 768)\n",
      "=== End Initialization ===\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([1722, 1528, 2900,  656]) As days turned into\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([1528, 2900,  656, 2745])  days turned into weeks\n",
      "\n",
      "=== Embedder Forward Pass ===\n",
      "\n",
      "embeddings[0] for x (4 x 768):\n",
      " tensor([[-0.6743,  0.2851,  0.5607,  ..., -0.3712,  0.6174, -0.5773],\n",
      "        [ 0.9675,  0.3970,  0.0029,  ...,  0.7599,  0.8818, -0.1195],\n",
      "        [-1.1204,  1.5178, -0.5717,  ...,  0.1642, -0.5433,  0.8593],\n",
      "        [ 0.5127, -0.5098,  0.7294,  ..., -0.8692, -0.6278, -0.3639]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 768):\n",
      " tensor([[-0.3834,  1.2171,  0.0605,  ..., -0.9916,  0.5717,  0.5908],\n",
      "        [-0.0440,  0.6047, -0.7804,  ..., -1.8195,  2.0859, -0.1849],\n",
      "        [ 0.0617,  3.1600,  0.1060,  ..., -0.7261,  0.8990,  0.2640],\n",
      "        [ 1.0921, -1.5200, -1.1416,  ..., -0.8093,  1.4341, -2.5053]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 768])\n",
      "=== End Forward Pass ===\n",
      "\n",
      "\n",
      "=== Transformer Initialization ===\n",
      "\n",
      "=== MultiHeadAttention Initialization ===\n",
      "    input_dim = 768\n",
      "    output_dim = 768\n",
      "    num_heads = 12\n",
      "    head_dim = 64\n",
      "    Generating nn.Linear(768, 768) weights for query, key and value\n",
      "    Generating causal diagonal mask torch.triu(torch.ones(1024, 1024), diagonal=1) for causal masking of attn_scores\n",
      "    Generating dropout nn.Dropout(0.1) for random dropout of attn_weights\n",
      "    Generating optional nn.Linear(768, 768) weights for final context_vector projection\n",
      "=== End MultiHeadAttention Initialization ===\n",
      "\n",
      "\n",
      "=== FeedForward Initialization ===\n",
      "    Input and output dimensions =  768\n",
      "    Hidden dimension =  3072\n",
      "=== End FeedForward Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "\n",
      "=== LayerNorm Initialization ===\n",
      "    embed_dim = 768\n",
      "    Generating self.shift = nn.Parameter(torch.zeros(768))\n",
      "    Generating self.scale = nn.Parameter(torch.ones(768))\n",
      "=== End LayerNorm Initialization ===\n",
      "\n",
      "Dropout rate:  0.1\n",
      "\n",
      "=== End Transformer Initialization ===\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([4, 768])):\n",
      "States for first batch ...\n",
      " tensor([[-1.0577,  1.5022,  0.6212,  ..., -1.3629,  1.1891,  0.0135],\n",
      "        [ 0.9235,  1.0017, -0.7775,  ..., -1.0596,  2.9677, -0.3044],\n",
      "        [-1.0587,  4.6778, -0.4657,  ..., -0.5619,  0.3557,  1.1233],\n",
      "        [ 1.6049, -2.0298, -0.4123,  ..., -1.6785,  0.8063, -2.8692]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[-0.7232,  1.0827,  0.4612,  ..., -0.9385,  0.8618,  0.0325],\n",
      "        [ 0.6569,  0.7145, -0.5964,  ..., -0.8043,  2.1631, -0.2479],\n",
      "        [-0.6795,  3.2676, -0.2715,  ..., -0.3377,  0.2937,  0.8218],\n",
      "        [ 1.1617, -1.5053, -0.3184,  ..., -1.2475,  0.5757, -2.1212]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([8, 4, 768]) (batch_size=8, context_length=4, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([4, 768])):\n",
      " tensor([[-0.7232,  1.0827,  0.4612,  ..., -0.9385,  0.8618,  0.0325],\n",
      "        [ 0.6569,  0.7145, -0.5964,  ..., -0.8043,  2.1631, -0.2479],\n",
      "        [-0.6795,  3.2676, -0.2715,  ..., -0.3377,  0.2937,  0.8218],\n",
      "        [ 1.1617, -1.5053, -0.3184,  ..., -1.2475,  0.5757, -2.1212]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([8, 4, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([8, 4, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([8, 12, 4, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([8, 12, 4, 4])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[-1.7295, -1.0389, -1.2549, -1.3818],\n",
      "        [ 4.5911,  3.3291,  4.0017, -4.5853],\n",
      "        [ 2.8569, -1.3261,  0.6317,  2.6327],\n",
      "        [ 1.7609, -3.1073, -2.1644,  2.3187]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[-1.7295,    -inf,    -inf,    -inf],\n",
      "        [ 4.5911,  3.3291,    -inf,    -inf],\n",
      "        [ 2.8569, -1.3261,  0.6317,    -inf],\n",
      "        [ 1.7609, -3.1073, -2.1644,  2.3187]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([8, 12, 4, 4])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5394, 0.4606, 0.0000, 0.0000],\n",
      "        [0.4255, 0.2523, 0.3222, 0.0000],\n",
      "        [0.3097, 0.1685, 0.1896, 0.3321]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1.], grad_fn=<SumBackward1>)\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5993, 0.5118, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2803, 0.3580, 0.0000],\n",
      "        [0.3441, 0.0000, 0.2107, 0.3690]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([8, 4, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([-1.0316, -0.6136, -1.4830,  ..., -0.5121,  0.9916, -0.8059],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([8, 4, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[-1.0316, -0.6136, -1.4830,  ...,  0.9276, -0.4889, -0.3153],\n",
      "        [-0.6206, -0.2598, -0.9994,  ...,  1.1157, -0.3024, -0.1652],\n",
      "        [ 0.2352,  0.1596, -0.1356,  ...,  0.5881, -0.0370,  0.0047],\n",
      "        [-0.1300, -0.5903, -0.1486,  ..., -0.2248, -0.3170,  0.1796]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([8, 4, 768])\n",
      "   Final context vector (batch 0): tensor([[ 0.3049, -0.6248,  0.1046,  ..., -0.1140, -0.2171,  0.4871],\n",
      "        [ 0.3453, -0.2101, -0.2001,  ..., -0.1388,  0.1009,  0.1301],\n",
      "        [ 0.1195, -0.0397,  0.0959,  ..., -0.1680, -0.1563, -0.0461],\n",
      "        [ 0.2701, -0.4378, -0.0254,  ...,  0.0310,  0.0593, -0.0522]],\n",
      "       grad_fn=<SelectBackward0>)...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[ 0.3049, -0.6248,  0.1046,  ..., -0.1140, -0.2171,  0.4871],\n",
      "        [ 0.3453, -0.2101, -0.2001,  ..., -0.1388,  0.1009,  0.1301],\n",
      "        [ 0.1195, -0.0397,  0.0959,  ..., -0.1680, -0.1563, -0.0461],\n",
      "        [ 0.2701, -0.4378, -0.0254,  ...,  0.0310,  0.0593, -0.0522]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[ 0.3388, -0.6942,  0.1162,  ..., -0.1267, -0.2412,  0.5412],\n",
      "        [ 0.3837, -0.2335, -0.2223,  ..., -0.1542,  0.1122,  0.1446],\n",
      "        [ 0.1328, -0.0441,  0.1065,  ..., -0.1867, -0.1737, -0.0513],\n",
      "        [ 0.3002, -0.4865, -0.0282,  ...,  0.0345,  0.0659, -0.0580]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[-0.7189,  0.8080,  0.7374,  ..., -1.4895,  0.9479,  0.5547],\n",
      "        [ 1.3072,  0.7682, -0.9998,  ..., -1.2138,  3.0799, -0.1598],\n",
      "        [-0.9259,  4.6336, -0.3592,  ..., -0.7486,  0.1820,  1.0720],\n",
      "        [ 1.9050, -2.5162, -0.4404,  ..., -1.6440,  0.8721, -2.9273]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[-0.4733,  0.5634,  0.5154,  ..., -0.9965,  0.6583,  0.3914],\n",
      "        [ 0.9130,  0.5252, -0.7469,  ..., -0.9008,  2.1885, -0.1425],\n",
      "        [-0.5862,  3.2303, -0.1972,  ..., -0.4645,  0.1744,  0.7853],\n",
      "        [ 1.3498, -1.8362, -0.3404,  ..., -1.2077,  0.6055, -2.1324]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[ 0.1085,  0.0060,  0.1963,  ...,  0.0160, -0.3011,  0.1167],\n",
      "        [-0.1118,  0.1984, -0.1401,  ..., -0.0773, -0.2713,  0.0812],\n",
      "        [ 0.4146,  0.2755,  0.1190,  ..., -0.1835, -0.1659, -0.2940],\n",
      "        [ 0.2001,  0.0213, -0.2903,  ...,  0.0718,  0.1539,  0.1265]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[ 0.1206,  0.0000,  0.0000,  ...,  0.0178, -0.3346,  0.1296],\n",
      "        [-0.0000,  0.2204, -0.1556,  ..., -0.0859, -0.3014,  0.0902],\n",
      "        [ 0.4607,  0.0000,  0.0000,  ..., -0.2039, -0.1844, -0.3267],\n",
      "        [ 0.2223,  0.0236, -0.3225,  ...,  0.0798,  0.1710,  0.1405]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[-5.9831e-01,  8.0800e-01,  7.3739e-01,  ..., -1.4718e+00,\n",
      "          6.1328e-01,  6.8430e-01],\n",
      "        [ 1.3072e+00,  9.8861e-01, -1.1555e+00,  ..., -1.2997e+00,\n",
      "          2.7785e+00, -6.9661e-02],\n",
      "        [-4.6520e-01,  4.6336e+00, -3.5919e-01,  ..., -9.5245e-01,\n",
      "         -2.3208e-03,  7.4534e-01],\n",
      "        [ 2.1273e+00, -2.4926e+00, -7.6293e-01,  ..., -1.5642e+00,\n",
      "          1.0431e+00, -2.7867e+00]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "Output shape:  torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"05. TransformerBlock.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d92f2",
   "metadata": {},
   "source": [
    "## GPT Model\n",
    "* 12 Transformers (n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = cfg[\"vocab_size\"]\n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "        self.n_layers = cfg[\"n_layers\"]\n",
    "\n",
    "        self.tok_emb = nn.Embedding(self.vocab_size, self.embbed_dim)\n",
    "        self.pos_emb = nn.Embedding(self.context_length, self.embbed_dim)\n",
    "        self.drob_emb = nn.Dropout(self.drop_rate)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.final_norm = LayerNorm(self.embbed_dim)\n",
    "        self.out_head = nn.Linear(self.embbed_dim, self.vocab_size, bias=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Initialization ===\")\n",
    "            print(f\"    vocab_size =\", self.vocab_size)\n",
    "            print(f\"    embbed_dim =\", self.embbed_dim)\n",
    "            print(f\"    context_length =\", self.context_length)\n",
    "            print(f\"    drop_rate =\", self.drop_rate)\n",
    "            print(f\"    n_layers =\", self.n_layers, \" (number of Tranformer blocks)\")\n",
    "            print(f\"    Generating nn.Embedding({self.vocab_size}, {self.embbed_dim}) weights for tokenID to embedding projection\")\n",
    "            print(f\"    Generating nn.Embedding({self.context_length}, {self.embbed_dim}) weights for positional encoding\")\n",
    "            print(f\"    Generating nn.Dropout({self.drop_rate})\")\n",
    "            print(f\"    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range({self.n_layers})])\")\n",
    "            print(f\"    Generating LayerNorm({self.embbed_dim}) for final normalization\")\n",
    "            print(f\"    Generating out_head nn.Linear({self.embbed_dim}, {self.vocab_size}, bias=False) for final output generation\")\n",
    "            print(f\"=== END GPTModel Initialization ===\\n\")\n",
    "            \n",
    "\n",
    "    def forward(self, token_ids, verbose=False):\n",
    "        \n",
    "        # local variables for input shape\n",
    "        batch_size, context_length = token_ids.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Forward Pass ===\")\n",
    "            print(f\"Input shape: {token_ids.shape} (batch_size={batch_size}, context_length={context_length})\")\n",
    "            \n",
    "        tok_emb = self.tok_emb(token_ids)\n",
    "        pos_emb = self.pos_emb(torch.arange(context_length, device=token_ids.device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        if verbose:\n",
    "            print(f'\\ntok_emb[0] for x ({context_length} x {tok_emb.shape[-1]}):\\n', tok_emb[0])\n",
    "            print(f'\\npos_emb[0] ({context_length} x {pos_emb.shape[-1]}):\\n', pos_emb)\n",
    "            print(\"\\nx[0] = tok_emb[0] + pos_emb[0]:\\n\", x[0])\n",
    "            print(\"\\nShape for input_embeddings: batch, context, embedding_dim \", x.shape)\n",
    "\n",
    "        x = self.drob_emb(x)\n",
    "        if verbose: print(f'\\nDropout on embedding:\\n', x[0])\n",
    "        \n",
    "        x = self.trf_blocks[0](x, verbose=verbose) # Just generate verbose output for first block\n",
    "        x = self.trf_blocks[1:](x)\n",
    "        if verbose: print(f'\\nAfter trf_blocks (only shows output for first one):\\n', x[0])\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        if verbose: print(f'\\nAfter final_norm:\\n', x[0])\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        if verbose: print(f'\\nFinal logits from out_head:\\n', x[0])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f2257",
   "metadata": {},
   "source": [
    "## Helper for context cropping and predicted token concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8d9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, token_IDs, max_new_tokens, context_size, verbose):\n",
    "    \n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop stored context exceeds context_size\n",
    "        token_ID_context = token_IDs[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(token_ID_context, verbose)   # GPTModel\n",
    "\n",
    "        # (batch, n_token, vocab_size) -> (batch, vocab_size) last token only\n",
    "        logits = logits[:, -1, :]\n",
    "        predicted_token = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        token_IDs = torch.cat((token_IDs, predicted_token), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "        if verbose: print(\"\\nStopping output after first run!!\\n\")\n",
    "        verbose = False # Stop output after first run\n",
    "\n",
    "    return token_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1019e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPTModel Initialization ===\n",
      "    vocab_size = 50257\n",
      "    embbed_dim = 768\n",
      "    context_length = 1024\n",
      "    drop_rate = 0.1\n",
      "    n_layers = 12  (number of Tranformer blocks)\n",
      "    Generating nn.Embedding(50257, 768) weights for tokenID to embedding projection\n",
      "    Generating nn.Embedding(1024, 768) weights for positional encoding\n",
      "    Generating nn.Dropout(0.1)\n",
      "    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(12)])\n",
      "    Generating LayerNorm(768) for final normalization\n",
      "    Generating out_head nn.Linear(768, 50257, bias=False) for final output generation\n",
      "=== END GPTModel Initialization ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello world! I'm \n",
      "Encoded input text: [15496, 995, 0, 314, 1101, 220]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPTModel Forward Pass ===\n",
      "Input shape: torch.Size([1, 6]) (batch_size=1, context_length=6)\n",
      "\n",
      "tok_emb[0] for x (6 x 768):\n",
      " tensor([[ 1.7279,  0.8710,  0.4013,  ..., -0.5229,  0.9800,  0.4260],\n",
      "        [ 0.5697,  0.0478,  1.5667,  ...,  0.4761,  0.5689,  1.7173],\n",
      "        [ 1.9269,  1.4873,  0.9007,  ..., -1.6034, -0.4298,  0.5762],\n",
      "        [-0.9255,  0.1910, -0.6354,  ..., -0.2358, -2.4088,  1.0164],\n",
      "        [ 0.7035, -0.9287,  0.6803,  ...,  0.7056, -0.0559,  0.2504],\n",
      "        [-0.5075, -1.2186, -0.3109,  ..., -1.1452,  0.8055, -0.2866]])\n",
      "\n",
      "pos_emb[0] (6 x 768):\n",
      " tensor([[-0.4581, -0.9307, -1.4735,  ..., -0.0823,  0.7471, -0.4912],\n",
      "        [-0.4950, -0.2879,  0.8584,  ..., -1.8412, -1.4230, -0.8485],\n",
      "        [-1.0269,  0.7796, -0.5159,  ...,  0.6624, -2.3840, -1.2934],\n",
      "        [ 0.3572,  0.3089, -1.2034,  ..., -0.5056,  0.3480,  0.2248],\n",
      "        [-1.4783, -0.6461, -0.8025,  ..., -0.7246,  0.8098,  0.4442],\n",
      "        [ 1.0714, -0.2934, -1.2601,  ..., -0.3944,  0.0824, -0.9926]])\n",
      "\n",
      "x[0] = tok_emb[0] + pos_emb[0]:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([1, 6, 768])\n",
      "\n",
      "Dropout on embedding:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([6, 768])):\n",
      "States for first batch ...\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([6, 768])):\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([1, 6, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([1, 6, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([1, 12, 6, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([1, 12, 6, 6])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,  1.4970, -2.2999, -2.0086,  5.4717,  1.3821],\n",
      "        [ 0.9614, -1.3905,  0.3245,  1.8619,  3.7510,  0.9524],\n",
      "        [-3.3369, -1.6342,  4.9844,  4.5802,  0.6181,  0.1094],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294, -2.7806, -0.6357],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,  1.3689],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.9614, -1.3905,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3369, -1.6342,  4.9844,    -inf,    -inf,    -inf],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294,    -inf,    -inf],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,    -inf],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([1, 12, 6, 6])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([1, 6, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([ 0.0482, -0.0312, -0.5868,  ...,  0.4167, -0.0060, -0.4255])...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([1, 6, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[ 0.0482, -0.0312, -0.5868,  ..., -0.3358,  1.0899,  0.5331],\n",
      "        [ 0.3551,  0.1795, -0.1324,  ..., -0.0359,  0.6439,  0.2635],\n",
      "        [ 0.3727,  0.1985, -0.2721,  ...,  0.1486,  0.3216, -0.1415],\n",
      "        [ 0.3652,  0.1764, -0.3196,  ...,  0.1169,  0.3826, -0.0981],\n",
      "        [ 0.6053,  0.3005, -0.1032,  ...,  0.2242,  0.2433, -0.4501],\n",
      "        [ 0.4930,  0.1845,  0.1123,  ...,  0.1846,  0.1185, -0.3963]])...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([1, 6, 768])\n",
      "   Final context vector (batch 0): tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[ 1.1073, -0.4185, -0.9625,  ..., -0.8392,  1.8598, -0.2965],\n",
      "        [-0.1026, -0.2067,  2.4489,  ..., -1.7178, -0.5014,  0.7669],\n",
      "        [ 0.8852,  2.4650,  0.3551,  ..., -0.9330, -2.6520, -0.6901],\n",
      "        [-0.5362,  0.5896, -1.7873,  ..., -0.7952, -1.8963,  1.2599],\n",
      "        [-0.7776, -1.5531, -0.0385,  ..., -0.1328,  0.8770,  0.8302],\n",
      "        [ 0.5251, -1.5912, -1.5416,  ..., -1.5737,  0.9813, -1.1160]])\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[ 0.7218, -0.3365, -0.7138,  ..., -0.6283,  1.2437, -0.2519],\n",
      "        [-0.0361, -0.1084,  1.7339,  ..., -1.1567, -0.3128,  0.5670],\n",
      "        [ 0.6596,  1.8227,  0.2694,  ..., -0.6789, -1.9444, -0.5001],\n",
      "        [-0.3203,  0.4646, -1.1926,  ..., -0.5009, -1.2686,  0.9320],\n",
      "        [-0.5530, -1.1000, -0.0317,  ..., -0.0982,  0.6140,  0.5810],\n",
      "        [ 0.3189, -1.1387, -1.1045,  ..., -1.1266,  0.6330, -0.8114]])\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[ 1.1436, -0.3361, -0.8282,  ..., -0.6732,  1.8953, -0.4511],\n",
      "        [-0.3109, -0.4971,  2.6279,  ..., -1.8216, -0.6279,  1.1742],\n",
      "        [ 1.0114,  2.5256,  0.2543,  ..., -0.7832, -2.4822, -0.7028],\n",
      "        [-0.6347,  0.6449, -1.8707,  ..., -0.6089, -2.0470,  1.0813],\n",
      "        [-0.6183, -1.7148, -0.0462,  ..., -0.5896,  1.1612,  0.7098],\n",
      "        [ 0.5498, -1.7744, -1.0419,  ..., -1.5545,  0.9491, -1.0046]])\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "\n",
      "After trf_blocks (only shows output for first one):\n",
      " tensor([[ 1.3654,  1.2487, -2.9049,  ..., -0.1999,  4.1747, -1.7501],\n",
      "        [ 1.0937, -0.1226,  3.0932,  ..., -2.1878,  0.9710, -0.8910],\n",
      "        [ 1.8707,  3.5901,  0.0666,  ..., -0.5200, -1.4109, -1.2726],\n",
      "        [ 0.9452,  2.8518, -2.2788,  ..., -0.5590,  0.1213,  0.0571],\n",
      "        [-0.1088, -0.4659, -1.0458,  ..., -0.0128,  2.5604, -0.5232],\n",
      "        [ 2.1185,  0.5855, -2.6416,  ..., -2.6304,  2.8844, -1.8247]])\n",
      "\n",
      "After final_norm:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Final logits from out_head:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Stopping output after first run!!\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,   995,     0,  ..., 34565,  6960, 46593]])\n",
      "Output length: 11\n",
      "Output text: Hello world! I'm  Unemployment whworn Young apr\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def test_run(verbose = False):\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    model = GPTModel(GPT_CONFIG_124M, verbose=verbose)\n",
    "    model.eval() # Disable dropout etc.\n",
    "\n",
    "    start_context = \"Hello world! I'm \"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    print(f\"\\n{50*'='}\")\n",
    "    \n",
    "    out = generate_tokens(\n",
    "        model=model,\n",
    "        token_IDs=encoded_tensor,\n",
    "        max_new_tokens=5,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "    \n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\": _test_run = test_run(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1d51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

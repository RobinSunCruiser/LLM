{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for GPT_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225ec960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"05. TransformerBlock.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d92f2",
   "metadata": {},
   "source": [
    "## GPT Model\n",
    "A complete GPT-style language model implementation that transforms token IDs into next-token predictions through several stages:\n",
    "1. Embedding layer: Converts token IDs to learned embeddings and adds positional encodings\n",
    "2. Transformer stack: Processes embeddings through multiple TransformerBlocks (self-attention + feed-forward)\n",
    "3. Output head: Final LayerNorm followed by linear projection to vocabulary logits\n",
    "\n",
    "\n",
    "The model uses absolute positional embeddings, dropout regularization, and outputs raw logits for each token in the vocabulary, enabling autoregressive text generation by predicting the next token given previous context.\n",
    "* 12 Transformers (n_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = cfg[\"vocab_size\"]\n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "        self.n_layers = cfg[\"n_layers\"]\n",
    "\n",
    "        self.tok_emb = nn.Embedding(self.vocab_size, self.embbed_dim)\n",
    "        self.pos_emb = nn.Embedding(self.context_length, self.embbed_dim)\n",
    "        self.drob_emb = nn.Dropout(self.drop_rate)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.final_norm = LayerNorm(self.embbed_dim)\n",
    "        self.out_head = nn.Linear(self.embbed_dim, self.vocab_size, bias=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Initialization ===\")\n",
    "            print(f\"    vocab_size =\", self.vocab_size)\n",
    "            print(f\"    embbed_dim =\", self.embbed_dim)\n",
    "            print(f\"    context_length =\", self.context_length)\n",
    "            print(f\"    drop_rate =\", self.drop_rate)\n",
    "            print(f\"    n_layers =\", self.n_layers, \" (number of Tranformer blocks)\")\n",
    "            print(f\"    Generating nn.Embedding({self.vocab_size}, {self.embbed_dim}) weights for tokenID to embedding projection\")\n",
    "            print(f\"    Generating nn.Embedding({self.context_length}, {self.embbed_dim}) weights for positional encoding\")\n",
    "            print(f\"    Generating nn.Dropout({self.drop_rate})\")\n",
    "            print(f\"    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range({self.n_layers})])\")\n",
    "            print(f\"    Generating LayerNorm({self.embbed_dim}) for final normalization\")\n",
    "            print(f\"    Generating out_head nn.Linear({self.embbed_dim}, {self.vocab_size}, bias=False) for final output generation\")\n",
    "            print(f\"=== END GPTModel Initialization ===\\n\")\n",
    "            \n",
    "\n",
    "    def forward(self, token_ids, verbose=False):\n",
    "        \n",
    "        # local variables for input shape\n",
    "        batch_size, context_length = token_ids.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Forward Pass ===\")\n",
    "            print(f\"Input shape: {token_ids.shape} (batch_size={batch_size}, context_length={context_length})\")\n",
    "            \n",
    "        tok_emb = self.tok_emb(token_ids)\n",
    "        pos_emb = self.pos_emb(torch.arange(context_length, device=token_ids.device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        if verbose:\n",
    "            print(f'\\ntok_emb[0] for x ({context_length} x {tok_emb.shape[-1]}):\\n', tok_emb[0])\n",
    "            print(f'\\npos_emb[0] ({context_length} x {pos_emb.shape[-1]}):\\n', pos_emb)\n",
    "            print(\"\\nx[0] = tok_emb[0] + pos_emb[0]:\\n\", x[0])\n",
    "            print(\"\\nShape for input_embeddings: batch, context, embedding_dim \", x.shape)\n",
    "\n",
    "        x = self.drob_emb(x)\n",
    "        if verbose: print(f'\\nDropout on embedding:\\n', x[0])\n",
    "        \n",
    "        x = self.trf_blocks[0](x, verbose=verbose) # Just generate verbose output for first block\n",
    "        x = self.trf_blocks[1:](x)\n",
    "        if verbose: print(f'\\nAfter trf_blocks (only shows output for first one):\\n', x[0])\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        if verbose: print(f'\\nAfter final_norm:\\n', x[0])\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        if verbose: print(f'\\nFinal logits from out_head:\\n', x[0])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f2257",
   "metadata": {},
   "source": [
    "## Demo helper for context cropping and predicted token concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8d9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, token_IDs, max_new_tokens, context_size, verbose = False):\n",
    "    \n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop stored context exceeds context_size\n",
    "        token_ID_context = token_IDs[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(token_ID_context, verbose)   # GPTModel\n",
    "\n",
    "        # (batch, n_token, vocab_size) -> (batch, vocab_size) last token only\n",
    "        logits = logits[:, -1, :]\n",
    "        predicted_token = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        token_IDs = torch.cat((token_IDs, predicted_token), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "        if verbose: print(\"\\nStopping output after first run!!\\n\")\n",
    "        verbose = False # Stop output after first run\n",
    "\n",
    "    return token_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd52c7",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1019e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPTModel Initialization ===\n",
      "    vocab_size = 50257\n",
      "    embbed_dim = 768\n",
      "    context_length = 1024\n",
      "    drop_rate = 0.1\n",
      "    n_layers = 12  (number of Tranformer blocks)\n",
      "    Generating nn.Embedding(50257, 768) weights for tokenID to embedding projection\n",
      "    Generating nn.Embedding(1024, 768) weights for positional encoding\n",
      "    Generating nn.Dropout(0.1)\n",
      "    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(12)])\n",
      "    Generating LayerNorm(768) for final normalization\n",
      "    Generating out_head nn.Linear(768, 50257, bias=False) for final output generation\n",
      "=== END GPTModel Initialization ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello world! I'm \n",
      "Encoded input text: [15496, 995, 0, 314, 1101, 220]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPTModel Forward Pass ===\n",
      "Input shape: torch.Size([1, 6]) (batch_size=1, context_length=6)\n",
      "\n",
      "tok_emb[0] for x (6 x 768):\n",
      " tensor([[ 1.7279,  0.8710,  0.4013,  ..., -0.5229,  0.9800,  0.4260],\n",
      "        [ 0.5697,  0.0478,  1.5667,  ...,  0.4761,  0.5689,  1.7173],\n",
      "        [ 1.9269,  1.4873,  0.9007,  ..., -1.6034, -0.4298,  0.5762],\n",
      "        [-0.9255,  0.1910, -0.6354,  ..., -0.2358, -2.4088,  1.0164],\n",
      "        [ 0.7035, -0.9287,  0.6803,  ...,  0.7056, -0.0559,  0.2504],\n",
      "        [-0.5075, -1.2186, -0.3109,  ..., -1.1452,  0.8055, -0.2866]])\n",
      "\n",
      "pos_emb[0] (6 x 768):\n",
      " tensor([[-0.4581, -0.9307, -1.4735,  ..., -0.0823,  0.7471, -0.4912],\n",
      "        [-0.4950, -0.2879,  0.8584,  ..., -1.8412, -1.4230, -0.8485],\n",
      "        [-1.0269,  0.7796, -0.5159,  ...,  0.6624, -2.3840, -1.2934],\n",
      "        [ 0.3572,  0.3089, -1.2034,  ..., -0.5056,  0.3480,  0.2248],\n",
      "        [-1.4783, -0.6461, -0.8025,  ..., -0.7246,  0.8098,  0.4442],\n",
      "        [ 1.0714, -0.2934, -1.2601,  ..., -0.3944,  0.0824, -0.9926]])\n",
      "\n",
      "x[0] = tok_emb[0] + pos_emb[0]:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([1, 6, 768])\n",
      "\n",
      "Dropout on embedding:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([6, 768])):\n",
      "States for first batch ...\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([6, 768])):\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([1, 6, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([1, 6, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([1, 12, 6, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([1, 12, 6, 6])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,  1.4970, -2.2999, -2.0086,  5.4717,  1.3821],\n",
      "        [ 0.9614, -1.3905,  0.3245,  1.8619,  3.7510,  0.9524],\n",
      "        [-3.3369, -1.6342,  4.9844,  4.5802,  0.6181,  0.1094],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294, -2.7806, -0.6357],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,  1.3689],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.9614, -1.3905,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3369, -1.6342,  4.9844,    -inf,    -inf,    -inf],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294,    -inf,    -inf],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,    -inf],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([1, 12, 6, 6])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([1, 6, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([ 0.0482, -0.0312, -0.5868,  ...,  0.4167, -0.0060, -0.4255])...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([1, 6, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[ 0.0482, -0.0312, -0.5868,  ..., -0.3358,  1.0899,  0.5331],\n",
      "        [ 0.3551,  0.1795, -0.1324,  ..., -0.0359,  0.6439,  0.2635],\n",
      "        [ 0.3727,  0.1985, -0.2721,  ...,  0.1486,  0.3216, -0.1415],\n",
      "        [ 0.3652,  0.1764, -0.3196,  ...,  0.1169,  0.3826, -0.0981],\n",
      "        [ 0.6053,  0.3005, -0.1032,  ...,  0.2242,  0.2433, -0.4501],\n",
      "        [ 0.4930,  0.1845,  0.1123,  ...,  0.1846,  0.1185, -0.3963]])...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([1, 6, 768])\n",
      "   Final context vector (batch 0): tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[ 1.1073, -0.4185, -0.9625,  ..., -0.8392,  1.8598, -0.2965],\n",
      "        [-0.1026, -0.2067,  2.4489,  ..., -1.7178, -0.5014,  0.7669],\n",
      "        [ 0.8852,  2.4650,  0.3551,  ..., -0.9330, -2.6520, -0.6901],\n",
      "        [-0.5362,  0.5896, -1.7873,  ..., -0.7952, -1.8963,  1.2599],\n",
      "        [-0.7776, -1.5531, -0.0385,  ..., -0.1328,  0.8770,  0.8302],\n",
      "        [ 0.5251, -1.5912, -1.5416,  ..., -1.5737,  0.9813, -1.1160]])\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[ 0.7218, -0.3365, -0.7138,  ..., -0.6283,  1.2437, -0.2519],\n",
      "        [-0.0361, -0.1084,  1.7339,  ..., -1.1567, -0.3128,  0.5670],\n",
      "        [ 0.6596,  1.8227,  0.2694,  ..., -0.6789, -1.9444, -0.5001],\n",
      "        [-0.3203,  0.4646, -1.1926,  ..., -0.5009, -1.2686,  0.9320],\n",
      "        [-0.5530, -1.1000, -0.0317,  ..., -0.0982,  0.6140,  0.5810],\n",
      "        [ 0.3189, -1.1387, -1.1045,  ..., -1.1266,  0.6330, -0.8114]])\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[ 1.1436, -0.3361, -0.8282,  ..., -0.6732,  1.8953, -0.4511],\n",
      "        [-0.3109, -0.4971,  2.6279,  ..., -1.8216, -0.6279,  1.1742],\n",
      "        [ 1.0114,  2.5256,  0.2543,  ..., -0.7832, -2.4822, -0.7028],\n",
      "        [-0.6347,  0.6449, -1.8707,  ..., -0.6089, -2.0470,  1.0813],\n",
      "        [-0.6183, -1.7148, -0.0462,  ..., -0.5896,  1.1612,  0.7098],\n",
      "        [ 0.5498, -1.7744, -1.0419,  ..., -1.5545,  0.9491, -1.0046]])\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "\n",
      "After trf_blocks (only shows output for first one):\n",
      " tensor([[ 1.3654,  1.2487, -2.9049,  ..., -0.1999,  4.1747, -1.7501],\n",
      "        [ 1.0937, -0.1226,  3.0932,  ..., -2.1878,  0.9710, -0.8910],\n",
      "        [ 1.8707,  3.5901,  0.0666,  ..., -0.5200, -1.4109, -1.2726],\n",
      "        [ 0.9452,  2.8518, -2.2788,  ..., -0.5590,  0.1213,  0.0571],\n",
      "        [-0.1088, -0.4659, -1.0458,  ..., -0.0128,  2.5604, -0.5232],\n",
      "        [ 2.1185,  0.5855, -2.6416,  ..., -2.6304,  2.8844, -1.8247]])\n",
      "\n",
      "After final_norm:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Final logits from out_head:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Stopping output after first run!!\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,   995,     0,  ..., 34565,  6960, 46593]])\n",
      "Output length: 11\n",
      "Output text: Hello world! I'm  Unemployment whworn Young apr\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def test_run(verbose = False):\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    model = GPTModel(GPT_CONFIG_124M, verbose=verbose)\n",
    "    model.eval() # Disable dropout etc.\n",
    "\n",
    "    start_context = \"Hello world! I'm \"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    print(f\"\\n{50*'='}\")\n",
    "    \n",
    "    out = generate_tokens(\n",
    "        model=model,\n",
    "        token_IDs=encoded_tensor,\n",
    "        max_new_tokens=5,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "    \n",
    "    return\n",
    "\n",
    "if '__file__' not in dir(): _test_run = test_run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1d51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for GPT_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225ec960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"03. Normalization.ipynb\"\n",
    "%run \"05. TransformerBlock.ipynb\"\n",
    "\n",
    "MultiHeadAttention = MultiHeadAttention\n",
    "LayerNorm = LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d92f2",
   "metadata": {},
   "source": [
    "## GPT Model\n",
    "* 12 Transformers (n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, verbose = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = cfg[\"vocab_size\"]\n",
    "        self.embbed_dim = cfg[\"emb_dim\"]\n",
    "        self.context_length = cfg[\"context_length\"]\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "        self.n_layers = cfg[\"n_layers\"]\n",
    "\n",
    "        self.tok_emb = nn.Embedding(self.vocab_size, self.embbed_dim)\n",
    "        self.pos_emb = nn.Embedding(self.context_length, self.embbed_dim)\n",
    "        self.drob_emb = nn.Dropout(self.drop_rate)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.final_norm = LayerNorm(self.embbed_dim)\n",
    "        self.out_head = nn.Linear(self.embbed_dim, self.vocab_size, bias=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Initialization ===\")\n",
    "            print(f\"    vocab_size =\", self.vocab_size)\n",
    "            print(f\"    embbed_dim =\", self.embbed_dim)\n",
    "            print(f\"    context_length =\", self.context_length)\n",
    "            print(f\"    drop_rate =\", self.drop_rate)\n",
    "            print(f\"    n_layers =\", self.n_layers, \" (number of Tranformer blocks)\")\n",
    "            print(f\"    Generating nn.Embedding({self.vocab_size}, {self.embbed_dim}) weights for tokenID to embedding projection\")\n",
    "            print(f\"    Generating nn.Embedding({self.context_length}, {self.embbed_dim}) weights for positional encoding\")\n",
    "            print(f\"    Generating nn.Dropout({self.drop_rate})\")\n",
    "            print(f\"    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range({self.n_layers})])\")\n",
    "            print(f\"    Generating LayerNorm({self.embbed_dim}) for final normalization\")\n",
    "            print(f\"    Generating out_head nn.Linear({self.embbed_dim}, {self.vocab_size}, bias=False) for final output generation\")\n",
    "            print(f\"=== END GPTModel Initialization ===\\n\")\n",
    "            \n",
    "\n",
    "    def forward(self, token_ids, verbose=False):\n",
    "        \n",
    "        # local variables for input shape\n",
    "        batch_size, context_length = token_ids.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== GPTModel Forward Pass ===\")\n",
    "            print(f\"Input shape: {token_ids.shape} (batch_size={batch_size}, context_length={context_length})\")\n",
    "            \n",
    "        tok_emb = self.tok_emb(token_ids)\n",
    "        pos_emb = self.pos_emb(torch.arange(context_length, device=token_ids.device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        if verbose:\n",
    "            print(f'\\ntok_emb[0] for x ({context_length} x {tok_emb.shape[-1]}):\\n', tok_emb[0])\n",
    "            print(f'\\npos_emb[0] ({context_length} x {pos_emb.shape[-1]}):\\n', pos_emb)\n",
    "            print(\"\\nx[0] = tok_emb[0] + pos_emb[0]:\\n\", x[0])\n",
    "            print(\"\\nShape for input_embeddings: batch, context, embedding_dim \", x.shape)\n",
    "\n",
    "        x = self.drob_emb(x)\n",
    "        if verbose: print(f'\\nDropout on embedding:\\n', x[0])\n",
    "        \n",
    "        x = self.trf_blocks[0](x, verbose=verbose) # Just generate verbose output for first block\n",
    "        x = self.trf_blocks[1:](x)\n",
    "        if verbose: print(f'\\nAfter trf_blocks (only shows output for first one):\\n', x[0])\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        if verbose: print(f'\\nAfter final_norm:\\n', x[0])\n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        if verbose: print(f'\\nFinal logits from out_head:\\n', x[0])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f2257",
   "metadata": {},
   "source": [
    "## Helper for context cropping and predicted token concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8d9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, token_IDs, max_new_tokens, context_size, verbose):\n",
    "    \n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop stored context exceeds context_size\n",
    "        token_ID_context = token_IDs[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(token_ID_context, verbose)   # GPTModel\n",
    "\n",
    "        # (batch, n_token, vocab_size) -> (batch, vocab_size) last token only\n",
    "        logits = logits[:, -1, :]\n",
    "        predicted_token = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        token_IDs = torch.cat((token_IDs, predicted_token), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "        if verbose: print(\"\\nStopping output after first run!!\\n\")\n",
    "        verbose = False # Stop output after first run\n",
    "\n",
    "    return token_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1019e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPTModel Initialization ===\n",
      "    vocab_size = 50257\n",
      "    embbed_dim = 768\n",
      "    context_length = 1024\n",
      "    drop_rate = 0.1\n",
      "    n_layers = 12  (number of Tranformer blocks)\n",
      "    Generating nn.Embedding(50257, 768) weights for tokenID to embedding projection\n",
      "    Generating nn.Embedding(1024, 768) weights for positional encoding\n",
      "    Generating nn.Dropout(0.1)\n",
      "    Generating nn.Sequential(*[TransformerBlock(cfg=cfg) for _ in range(12)])\n",
      "    Generating LayerNorm(768) for final normalization\n",
      "    Generating out_head nn.Linear(768, 50257, bias=False) for final output generation\n",
      "=== END GPTModel Initialization ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello world! I'm \n",
      "Encoded input text: [15496, 995, 0, 314, 1101, 220]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== GPTModel Forward Pass ===\n",
      "Input shape: torch.Size([1, 6]) (batch_size=1, context_length=6)\n",
      "\n",
      "tok_emb[0] for x (6 x 768):\n",
      " tensor([[ 1.7279,  0.8710,  0.4013,  ..., -0.5229,  0.9800,  0.4260],\n",
      "        [ 0.5697,  0.0478,  1.5667,  ...,  0.4761,  0.5689,  1.7173],\n",
      "        [ 1.9269,  1.4873,  0.9007,  ..., -1.6034, -0.4298,  0.5762],\n",
      "        [-0.9255,  0.1910, -0.6354,  ..., -0.2358, -2.4088,  1.0164],\n",
      "        [ 0.7035, -0.9287,  0.6803,  ...,  0.7056, -0.0559,  0.2504],\n",
      "        [-0.5075, -1.2186, -0.3109,  ..., -1.1452,  0.8055, -0.2866]])\n",
      "\n",
      "pos_emb[0] (6 x 768):\n",
      " tensor([[-0.4581, -0.9307, -1.4735,  ..., -0.0823,  0.7471, -0.4912],\n",
      "        [-0.4950, -0.2879,  0.8584,  ..., -1.8412, -1.4230, -0.8485],\n",
      "        [-1.0269,  0.7796, -0.5159,  ...,  0.6624, -2.3840, -1.2934],\n",
      "        [ 0.3572,  0.3089, -1.2034,  ..., -0.5056,  0.3480,  0.2248],\n",
      "        [-1.4783, -0.6461, -0.8025,  ..., -0.7246,  0.8098,  0.4442],\n",
      "        [ 1.0714, -0.2934, -1.2601,  ..., -0.3944,  0.0824, -0.9926]])\n",
      "\n",
      "x[0] = tok_emb[0] + pos_emb[0]:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([1, 6, 768])\n",
      "\n",
      "Dropout on embedding:\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "=== TransformerBlock Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, embbed_dim=768\n",
      "\n",
      "Input tensor (=shortcut) (batch 0 with shape torch.Size([6, 768])):\n",
      "States for first batch ...\n",
      " tensor([[ 1.2698, -0.0597, -1.0722,  ..., -0.6052,  1.7270, -0.0652],\n",
      "        [ 0.0747, -0.2401,  2.4252,  ..., -1.3652, -0.8542,  0.8688],\n",
      "        [ 0.9000,  2.2669,  0.3848,  ..., -0.9410, -2.8139, -0.7172],\n",
      "        [-0.5682,  0.4999, -1.8388,  ..., -0.7414, -2.0608,  1.2412],\n",
      "        [-0.7748, -1.5748, -0.1223,  ..., -0.0190,  0.7539,  0.6946],\n",
      "        [ 0.5638, -1.5120, -1.5709,  ..., -1.5396,  0.8878, -1.2793]])\n",
      "\n",
      "1. Normalization 1:\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "=== MultiHeadAttention Forward Pass ===\n",
      "Input shape: torch.Size([1, 6, 768]) (batch_size=1, context_length=6, input_dim=768)\n",
      "Config: num_heads=12, head_dim=64, output_dim=768\n",
      "\n",
      "Input tensor (batch 0 with shape torch.Size([6, 768])):\n",
      " tensor([[ 0.8664, -0.0787, -0.7984,  ..., -0.4664,  1.1914, -0.0826],\n",
      "        [ 0.0880, -0.1335,  1.7415,  ..., -0.9250, -0.5655,  0.6466],\n",
      "        [ 0.6736,  1.6865,  0.2919,  ..., -0.6906, -2.0785, -0.5248],\n",
      "        [-0.3433,  0.4077, -1.2366,  ..., -0.4651, -1.3927,  0.9289],\n",
      "        [-0.5622, -1.1336, -0.0961,  ..., -0.0223,  0.5298,  0.4874],\n",
      "        [ 0.3459, -1.0935, -1.1344,  ..., -1.1127,  0.5705, -0.9322]])\n",
      "\n",
      "1. QKV Projection:\n",
      "   QKV shapes: torch.Size([1, 6, 768])\n",
      "\n",
      "2. Split into heads:\n",
      "(batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
      "   QKV shapes after view: torch.Size([1, 6, 12, 64])\n",
      "\n",
      "3. Transpose for attention computation:\n",
      "(batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
      "   QKV shapes after transpose: torch.Size([1, 12, 6, 64])\n",
      "\n",
      "4. Attention scores computation:\n",
      "   attn_scores shape: torch.Size([1, 12, 6, 6])\n",
      "   Scale factor (1/sqrt(head_dim)): 0.1250\n",
      "   Raw attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,  1.4970, -2.2999, -2.0086,  5.4717,  1.3821],\n",
      "        [ 0.9614, -1.3905,  0.3245,  1.8619,  3.7510,  0.9524],\n",
      "        [-3.3369, -1.6342,  4.9844,  4.5802,  0.6181,  0.1094],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294, -2.7806, -0.6357],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,  1.3689],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "5. Causal masking:\n",
      "   Causal mask:\n",
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]])\n",
      "\n",
      "   Then masked_fill True -> -torch.inf\n",
      "\n",
      "   Masked attention scores for head 0, batch 0:\n",
      "tensor([[ 2.1764,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.9614, -1.3905,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3369, -1.6342,  4.9844,    -inf,    -inf,    -inf],\n",
      "        [ 2.1125, -2.1552, -0.9903, -3.6294,    -inf,    -inf],\n",
      "        [ 3.6141,  2.8531,  2.9374, -3.2974,  2.2013,    -inf],\n",
      "        [-1.8442, -3.0452,  0.2298, -6.3050, -1.5770,  1.9511]])\n",
      "\n",
      "6. Softmax attention weights:\n",
      "   attn_weights shape: torch.Size([1, 12, 6, 6])\n",
      "   Attention weights for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "   Sum of weights (should be ~1.0): tensor([1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "7. After dropout:\n",
      "   Attention weights after dropout for head 0, batch 0:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5730, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1974, 0.2442, 0.5585, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3632, 0.2131, 0.2465, 0.1772, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2224, 0.2248, 0.1031, 0.2050, 0.0000],\n",
      "        [0.1570, 0.1351, 0.2034, 0.0899, 0.1623, 0.2523]])\n",
      "\n",
      "8. Compute context vectors:\n",
      "   context_vec shape after attention: torch.Size([1, 6, 12, 64])\n",
      "   First context vector (batch 0, token 0, head 0): tensor([ 0.0482, -0.0312, -0.5868,  ...,  0.4167, -0.0060, -0.4255])...\n",
      "\n",
      "9. Concatenate heads:\n",
      "   context_vec shape after view: torch.Size([1, 6, 768])\n",
      "   First concatenated context vector (batch 0): tensor([[ 0.0482, -0.0312, -0.5868,  ..., -0.3358,  1.0899,  0.5331],\n",
      "        [ 0.3551,  0.1795, -0.1324,  ..., -0.0359,  0.6439,  0.2635],\n",
      "        [ 0.3727,  0.1985, -0.2721,  ...,  0.1486,  0.3216, -0.1415],\n",
      "        [ 0.3652,  0.1764, -0.3196,  ...,  0.1169,  0.3826, -0.0981],\n",
      "        [ 0.6053,  0.3005, -0.1032,  ...,  0.2242,  0.2433, -0.4501],\n",
      "        [ 0.4930,  0.1845,  0.1123,  ...,  0.1846,  0.1185, -0.3963]])...\n",
      "\n",
      "10. Final output projection:\n",
      "   Final context_vec shape: torch.Size([1, 6, 768])\n",
      "   Final context vector (batch 0): tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])...\n",
      "=== End MultiHeadAttention Forward Pass ===\n",
      "\n",
      "\n",
      "2. Attention:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "3. Dropout:\n",
      " tensor([[-0.1625, -0.3588,  0.1098,  ..., -0.2341,  0.1328, -0.2312],\n",
      "        [-0.1773,  0.0334,  0.0237,  ..., -0.3527,  0.3528, -0.1019],\n",
      "        [-0.0148,  0.1982, -0.0297,  ...,  0.0080,  0.1619,  0.0271],\n",
      "        [ 0.0320,  0.0897,  0.0515,  ..., -0.0538,  0.1645,  0.0187],\n",
      "        [-0.0028,  0.0217,  0.0838,  ..., -0.1138,  0.1231,  0.1356],\n",
      "        [-0.0387, -0.0793,  0.0294,  ..., -0.0341,  0.0934,  0.1632]])\n",
      "\n",
      "4. Output + Shortcut (= new Shortcut):\n",
      " tensor([[ 1.1073, -0.4185, -0.9625,  ..., -0.8392,  1.8598, -0.2965],\n",
      "        [-0.1026, -0.2067,  2.4489,  ..., -1.7178, -0.5014,  0.7669],\n",
      "        [ 0.8852,  2.4650,  0.3551,  ..., -0.9330, -2.6520, -0.6901],\n",
      "        [-0.5362,  0.5896, -1.7873,  ..., -0.7952, -1.8963,  1.2599],\n",
      "        [-0.7776, -1.5531, -0.0385,  ..., -0.1328,  0.8770,  0.8302],\n",
      "        [ 0.5251, -1.5912, -1.5416,  ..., -1.5737,  0.9813, -1.1160]])\n",
      "\n",
      "5. Normalization 2:\n",
      " tensor([[ 0.7218, -0.3365, -0.7138,  ..., -0.6283,  1.2437, -0.2519],\n",
      "        [-0.0361, -0.1084,  1.7339,  ..., -1.1567, -0.3128,  0.5670],\n",
      "        [ 0.6596,  1.8227,  0.2694,  ..., -0.6789, -1.9444, -0.5001],\n",
      "        [-0.3203,  0.4646, -1.1926,  ..., -0.5009, -1.2686,  0.9320],\n",
      "        [-0.5530, -1.1000, -0.0317,  ..., -0.0982,  0.6140,  0.5810],\n",
      "        [ 0.3189, -1.1387, -1.1045,  ..., -1.1266,  0.6330, -0.8114]])\n",
      "\n",
      "6. FeedForward:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "7. Dropout:\n",
      " tensor([[ 0.0363,  0.0825,  0.1343,  ...,  0.1660,  0.0355, -0.1547],\n",
      "        [-0.2083, -0.2904,  0.1791,  ..., -0.1037, -0.1265,  0.4073],\n",
      "        [ 0.1262,  0.0606, -0.1008,  ...,  0.1497,  0.1698, -0.0127],\n",
      "        [-0.0985,  0.0553, -0.0834,  ...,  0.1863, -0.1508, -0.1786],\n",
      "        [ 0.1592, -0.1617, -0.0077,  ..., -0.4568,  0.2842, -0.1204],\n",
      "        [ 0.0247, -0.1832,  0.4997,  ...,  0.0192, -0.0322,  0.1114]])\n",
      "\n",
      "8. Output + new Shortcut:\n",
      " tensor([[ 1.1436, -0.3361, -0.8282,  ..., -0.6732,  1.8953, -0.4511],\n",
      "        [-0.3109, -0.4971,  2.6279,  ..., -1.8216, -0.6279,  1.1742],\n",
      "        [ 1.0114,  2.5256,  0.2543,  ..., -0.7832, -2.4822, -0.7028],\n",
      "        [-0.6347,  0.6449, -1.8707,  ..., -0.6089, -2.0470,  1.0813],\n",
      "        [-0.6183, -1.7148, -0.0462,  ..., -0.5896,  1.1612,  0.7098],\n",
      "        [ 0.5498, -1.7744, -1.0419,  ..., -1.5545,  0.9491, -1.0046]])\n",
      "\n",
      "===END TransformerBlock Forward Pass ===\n",
      "\n",
      "\n",
      "After trf_blocks (only shows output for first one):\n",
      " tensor([[ 1.3654,  1.2487, -2.9049,  ..., -0.1999,  4.1747, -1.7501],\n",
      "        [ 1.0937, -0.1226,  3.0932,  ..., -2.1878,  0.9710, -0.8910],\n",
      "        [ 1.8707,  3.5901,  0.0666,  ..., -0.5200, -1.4109, -1.2726],\n",
      "        [ 0.9452,  2.8518, -2.2788,  ..., -0.5590,  0.1213,  0.0571],\n",
      "        [-0.1088, -0.4659, -1.0458,  ..., -0.0128,  2.5604, -0.5232],\n",
      "        [ 2.1185,  0.5855, -2.6416,  ..., -2.6304,  2.8844, -1.8247]])\n",
      "\n",
      "After final_norm:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Final logits from out_head:\n",
      " tensor([[ 0.6970,  0.6351, -1.5697,  ..., -0.1339,  2.1882, -0.9568],\n",
      "        [ 0.6619, -0.0265,  1.7936,  ..., -1.1954,  0.5925, -0.4614],\n",
      "        [ 1.1314,  2.1420,  0.0710,  ..., -0.2738, -0.7974, -0.7161],\n",
      "        [ 0.5630,  1.6227, -1.2287,  ..., -0.2730,  0.1052,  0.0695],\n",
      "        [-0.0424, -0.2504, -0.5883,  ...,  0.0135,  1.5128, -0.2838],\n",
      "        [ 1.1834,  0.3135, -1.5179,  ..., -1.5115,  1.6180, -1.0543]])\n",
      "\n",
      "Stopping output after first run!!\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,   995,     0,  ..., 34565,  6960, 46593]])\n",
      "Output length: 11\n",
      "Output text: Hello world! I'm  Unemployment whworn Young apr\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def test_run(verbose = False):\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    model = GPTModel(GPT_CONFIG_124M, verbose=verbose)\n",
    "    model.eval() # Disable dropout etc.\n",
    "\n",
    "    start_context = \"Hello world! I'm \"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    print(f\"\\n{50*'='}\")\n",
    "    \n",
    "    out = generate_tokens(\n",
    "        model=model,\n",
    "        token_IDs=encoded_tensor,\n",
    "        max_new_tokens=5,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "    \n",
    "    return\n",
    "\n",
    "if '__file__' not in dir(): _test_run = test_run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1d51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

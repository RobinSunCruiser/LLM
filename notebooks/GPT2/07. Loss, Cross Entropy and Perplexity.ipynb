{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5dcba",
   "metadata": {},
   "source": [
    "# Playground for Losses. Cross Entropy and Perplexity\n",
    "* Calculation steps for Cross Entropy\n",
    "* Calculation steps for Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285f56",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49d2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3, precision=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"01. DataPreparation.ipynb\"\n",
    "%run \"06. GPTModel.ipynb\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "# From 01. DataPreparation\n",
    "create_dataloader = create_dataloader\n",
    "\n",
    "# From 06. GPTModel\n",
    "model = GPTModel(GPT_CONFIG_124M) # From 06. GPTModel.ipynb\n",
    "model.eval();  # Disable dropout during inference\n",
    "generate_tokens = generate_tokens # From 06. GPTModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d79d",
   "metadata": {},
   "source": [
    "## Creating Input and Target set for demonstration\n",
    "* Generates test Batches Input + Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b01812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8])\n",
      "Target shape: torch.Size([2, 8])\n",
      "\n",
      "Input tokens:\n",
      " tensor([[ 4504,   284,  3498,  ...,   611,  2147,   550],\n",
      "        [  464,  1708,  2745,  ...,    13,   198, 23588]])\n",
      "\n",
      "Target tokens:\n",
      " tensor([[  284,  3498,  5433,  ...,  2147,   550,  3022],\n",
      "        [ 1708,  2745,   547,  ...,   198, 23588,   602]])\n",
      "\n",
      "Decoded inputs:\n",
      "  [0]:  returned to Lab 42 as if nothing had\n",
      "  [1]: The following weeks were chaos.\n",
      "Equ\n",
      "\n",
      "Decoded targets:\n",
      "  [0]:  to Lab 42 as if nothing had happened\n",
      "  [1]:  following weeks were chaos.\n",
      "Equations\n"
     ]
    }
   ],
   "source": [
    "def get_test_batch(batch_size=2, context_length=6, stride=4, verbose=False):\n",
    "    \n",
    "    # Load sample text and create dataloader\n",
    "    with open(\"00. Robins Small Text Sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read()\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataloader = create_dataloader(\n",
    "        raw_text, \n",
    "        tokenizer_model_name=\"gpt2\",\n",
    "        batch_size=batch_size, \n",
    "        context_length=context_length, \n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    # Get one batch of input and target data\n",
    "    batch = next(iter(dataloader))\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        print(\"Target shape:\", targets.shape)\n",
    "        print(\"\\nInput tokens:\\n\", inputs)\n",
    "        print(\"\\nTarget tokens:\\n\", targets)\n",
    "        print(\"\\nDecoded inputs:\")\n",
    "        for i in range(inputs.shape[0]):\n",
    "            print(f\"  [{i}]: {tokenizer.decode(inputs[i].tolist())}\")\n",
    "        print(\"\\nDecoded targets:\")\n",
    "        for i in range(targets.shape[0]):\n",
    "            print(f\"  [{i}]: {tokenizer.decode(targets[i].tolist())}\")\n",
    "    \n",
    "    return inputs, targets, tokenizer\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    inputs, targets, tokenizer = get_test_batch(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c77208",
   "metadata": {},
   "source": [
    "## Untrained model logits output (single forward pass)\n",
    "* Real output of the model are logits (unnormalized scores from output layer)\n",
    "* After applying softmax we get a distribution of probabilites over the whole vocabulary (values sum to 1)\n",
    "* Argmax on probabilities retuns most probable token_id (array index) for next token\n",
    "* Model calculates logits for all input positions in parallel (Transformer architecture), but only the last position's logits are used for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1ce10456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape (batch_size, num_tokens, vocab_size): torch.Size([2, 8, 50257])\n",
      "\n",
      "Returns logits for every partial sequence. Current end token as Q decodes next output\n",
      "In production only last logit is relevant. In training all can be used\n",
      "\n",
      "Probability vector for next word after every of the 8 input token (batch 0).\n",
      "Index = Token_ID\n",
      " tensor([[2.50e-05, 1.65e-05, 1.18e-05,  ..., 1.23e-05, 7.70e-06, 1.23e-05],\n",
      "        [1.33e-05, 2.51e-05, 1.23e-05,  ..., 7.07e-06, 1.28e-05, 8.43e-05],\n",
      "        [1.66e-05, 7.78e-06, 7.39e-06,  ..., 1.56e-05, 3.74e-06, 1.09e-05],\n",
      "        ...,\n",
      "        [1.07e-05, 4.19e-06, 1.29e-05,  ..., 2.23e-05, 9.73e-06, 2.86e-05],\n",
      "        [1.68e-05, 1.12e-05, 1.23e-05,  ..., 7.41e-06, 1.30e-05, 1.87e-05],\n",
      "        [2.25e-05, 1.19e-05, 5.59e-06,  ..., 1.88e-05, 7.56e-06, 1.84e-05]])\n",
      "\n",
      "ArgMax gives Index of vector = token_id with max value. Shape: torch.Size([2, 8])\n",
      "Token IDs (all batches):\n",
      " tensor([[37723, 34784, 30389,  ..., 49327, 30327, 30257],\n",
      "        [43794, 24782, 16450,  ..., 31853,   256,   404]])\n",
      "\n",
      "Batch [0]:\n",
      " ' returned'=>' Tara'[37723] ✗\n",
      " ' returned to'=>'Probably'[34784] ✗\n",
      " ' returned to Lab'=>' thighs'[30389] ✗\n",
      " ' returned to Lab 42'=>'375'[22318] ✗\n",
      " ' returned to Lab 42 as'=>'oku'[11601] ✗\n",
      " ' returned to Lab 42 as if'=>' 363'[49327] ✗\n",
      " ' returned to Lab 42 as if nothing'=>'Anth'[30327] ✗\n",
      " ' returned to Lab 42 as if nothing had'=>' vigilant'[30257] ✗\n",
      "\n",
      "Batch [1]:\n",
      " 'The'=>' GUN'[43794] ✗\n",
      " 'The following'=>' Championships'[24782] ✗\n",
      " 'The following weeks'=>' 1969'[16450] ✗\n",
      " 'The following weeks were'=>'onsequ'[40819] ✗\n",
      " 'The following weeks were chaos'=>' discontent'[39784] ✗\n",
      " 'The following weeks were chaos.'=>' barric'[31853] ✗\n",
      " 'The following weeks were chaos.\n",
      "'=>' t'[256] ✗\n",
      " 'The following weeks were chaos.\n",
      "Equ'=>'op'[404] ✗\n"
     ]
    }
   ],
   "source": [
    "def analyze_predictions(model, inputs, targets, tokenizer, verbose=False):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.argmax(probas, dim=-1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nOutput shape (batch_size, num_tokens, vocab_size):\", probas.shape)\n",
    "        print(\"\\nReturns logits for every partial sequence. Current end token as Q decodes next output\\n\"\n",
    "              \"In production only last logit is relevant. In training all can be used\")\n",
    "        print(f'\\nProbability vector for next word after every of the {probas.shape[1]} input token (batch 0).\\nIndex = Token_ID\\n', probas[0])\n",
    "        print(\"\\nArgMax gives Index of vector = token_id with max value. Shape:\", predicted_ids.shape)\n",
    "        print(\"Token IDs (all batches):\\n\", predicted_ids)\n",
    "        \n",
    "        # Show predictions vs targets for each batch item\n",
    "        for i in range(predicted_ids.shape[0]):\n",
    "            print(f\"\\nBatch [{i}]:\")\n",
    "            input_tokens = inputs[i].tolist()\n",
    "            predicted_tokens = predicted_ids[i].tolist()\n",
    "            target_tokens = targets[i].tolist()\n",
    "            \n",
    "            for pos in range(len(input_tokens)):\n",
    "                context = tokenizer.decode(input_tokens[:pos+1])\n",
    "                predicted = tokenizer.decode([predicted_tokens[pos]])\n",
    "                match = \"✓\" if predicted_tokens[pos] == target_tokens[pos] else \"✗\"\n",
    "                print(f\" '{context}'=>'{predicted}'[{predicted_tokens[pos]}] {match}\")\n",
    "    \n",
    "    return logits, probas, predicted_ids\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    logits, probas, predicted_ids = analyze_predictions(model, inputs, targets, tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1ce57",
   "metadata": {},
   "source": [
    "# Training Goal and Cross Entropy Loss\n",
    "* Goal: Maximize probability of correct target tokens (bring `probas[target_id]` close to 1.0)\n",
    "* Logarithms convert products to sums and prevent numerical underflow, making optimization easier\n",
    "* Cross-Entropy Loss = negative average log probability of target tokens: `-mean(log(probas[target_id]))`\n",
    "* Loss can be computed at different granularities:\n",
    "    * Per-token: `-log(P(y_i))` for individual predictions (batch_size, context_length)\n",
    "    * Per-sequence: Average over tokens in each sequence (batch_size,)\n",
    "    * Batch-average (common implemented here): Single scalar averaged over all predictions (N = batch_size × context_length)\n",
    "* Formula: `Loss = -1/N * Σ log(P(y_i))`\n",
    "    * `N` = total number of predictions being averaged over\n",
    "    * `y_i` = the i-th target token (correct token at position i)\n",
    "    * `P(y_i)` = model's predicted probability for target token y_i\n",
    "    * `Σ` = sum over all N predictions\n",
    "* Training typically uses batch-average loss for backpropagation\n",
    "* Lower loss = model assigns higher probability to correct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3aab44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log of probabilities (batch 0):\n",
      " tensor([[-10.60, -11.01, -11.35,  ..., -11.31, -11.77, -11.30],\n",
      "        [-11.23, -10.59, -11.30,  ..., -11.86, -11.26,  -9.38],\n",
      "        [-11.01, -11.76, -11.82,  ..., -11.07, -12.50, -11.43],\n",
      "        ...,\n",
      "        [-11.44, -12.38, -11.25,  ..., -10.71, -11.54, -10.46],\n",
      "        [-10.99, -11.40, -11.31,  ..., -11.81, -11.25, -10.89],\n",
      "        [-10.70, -11.34, -12.09,  ..., -10.88, -11.79, -10.90]])\n",
      "\n",
      "Log probabilities for target tokens only:\n",
      "Shape: torch.Size([2, 8])\n",
      "tensor([[ -9.83, -12.11, -11.36,  ..., -11.29, -10.38, -11.14],\n",
      "        [-10.17, -10.86, -10.67,  ...,  -9.97, -11.46, -11.59]])\n",
      "\n",
      "Average log probability: -10.8851\n",
      "Negative average log probability (loss): 10.8851\n",
      "\n",
      "Per-token log probabilities for targets:\n",
      "\n",
      "Batch [0]:\n",
      "  Position 0: target ' to' [284] -> log_prob: -9.8300\n",
      "  Position 1: target ' Lab' [3498] -> log_prob: -12.1084\n",
      "  Position 2: target ' 42' [5433] -> log_prob: -11.3566\n",
      "  Position 3: target ' as' [355] -> log_prob: -11.1432\n",
      "  Position 4: target ' if' [611] -> log_prob: -10.2771\n",
      "  Position 5: target ' nothing' [2147] -> log_prob: -11.2875\n",
      "  Position 6: target ' had' [550] -> log_prob: -10.3819\n",
      "  Position 7: target ' happened' [3022] -> log_prob: -11.1445\n",
      "\n",
      "Batch [1]:\n",
      "  Position 0: target ' following' [1708] -> log_prob: -10.1731\n",
      "  Position 1: target ' weeks' [2745] -> log_prob: -10.8622\n",
      "  Position 2: target ' were' [547] -> log_prob: -10.6712\n",
      "  Position 3: target ' chaos' [11918] -> log_prob: -11.2039\n",
      "  Position 4: target '.' [13] -> log_prob: -10.6985\n",
      "  Position 5: target '\n",
      "' [198] -> log_prob: -9.9741\n",
      "  Position 6: target 'Equ' [23588] -> log_prob: -11.4622\n",
      "  Position 7: target 'ations' [602] -> log_prob: -11.5875\n"
     ]
    }
   ],
   "source": [
    "def extract_target_log_probas(probas, targets, tokenizer, verbose=False):\n",
    "    # Compute logarithm of all token probabilities\n",
    "    log_probas = torch.log(probas)\n",
    "    \n",
    "    # Extract log probabilities only for target tokens\n",
    "    batch_size, num_tokens = targets.shape\n",
    "    target_log_probas = log_probas[torch.arange(batch_size).unsqueeze(1), torch.arange(num_tokens), targets]\n",
    "    \n",
    "    # Calculate the average log probability for target tokens\n",
    "    avg_log_probas = torch.mean(target_log_probas)\n",
    "    \n",
    "    # Negative average log probability (cross-entropy loss)\n",
    "    cross_entropy_loss = avg_log_probas * -1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Log of probabilities (batch 0):\\n\", log_probas[0])\n",
    "        print(\"\\nLog probabilities for target tokens only:\")\n",
    "        print(\"Shape:\", target_log_probas.shape)\n",
    "        print(target_log_probas)\n",
    "        print(f\"\\nAverage log probability: {avg_log_probas.item():.4f}\")\n",
    "        print(f\"Negative average log probability (loss): {cross_entropy_loss.item():.4f}\")\n",
    "        \n",
    "        # Show per-token breakdown\n",
    "        print(\"\\nPer-token log probabilities for targets:\")\n",
    "        for i in range(batch_size):\n",
    "            print(f\"\\nBatch [{i}]:\")\n",
    "            for pos in range(num_tokens):\n",
    "                target_token = targets[i, pos].item()\n",
    "                log_prob = target_log_probas[i, pos].item()\n",
    "                target_text = tokenizer.decode([target_token])\n",
    "                print(f\"  Position {pos}: target '{target_text}' [{target_token}] -> log_prob: {log_prob:.4f}\")\n",
    "    \n",
    "    return target_log_probas, avg_log_probas, cross_entropy_loss\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    target_log_probas, avg_log_probas, cross_entropy_loss = extract_target_log_probas(probas, targets, tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328cc08",
   "metadata": {},
   "source": [
    "## Use Pytorch Calculation function\n",
    "* Takes care of softmax, log-probability etc.\n",
    "* Batch dimensions need to be flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f68774de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy loss: 10.8851\n",
      "PyTorch cross-entropy loss: 10.8851\n"
     ]
    }
   ],
   "source": [
    "# Use Pytorch Calculation function\n",
    "if '__file__' not in dir():\n",
    "    # Manual calculation\n",
    "    _, _, manual_loss = extract_target_log_probas(probas, targets, tokenizer, verbose=False)\n",
    "    \n",
    "    # PyTorch CrossEntropyLoss\n",
    "    # Flatten: (batch, context, vocab) -> (batch*context, vocab) and (batch, context) -> (batch*context)\n",
    "    logits_flat = logits.flatten(0, 1)  # Flatten first 2 dims (start, end)\n",
    "    targets_flat = targets.flatten()     # Flatten all dims\n",
    "    pytorch_loss = nn.CrossEntropyLoss()(logits_flat, targets_flat)\n",
    "    \n",
    "    print(f\"Manual cross-entropy loss: {manual_loss.item():.4f}\")\n",
    "    print(f\"PyTorch cross-entropy loss: {pytorch_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f18ae",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "* Simply exponential of cross-entropy loss\n",
    "* Considered more interpratable \n",
    "    * Can be understood as en estimate of which numbers of vocabulary a model is uncertain about\n",
    "    * How well the probability predicted by model matches actual distribution in words of dataset\n",
    "    * Lower perplexity -> better model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5399b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  tensor(53377.07)\n",
      "Vocabulary:  50257\n"
     ]
    }
   ],
   "source": [
    "if '__file__' not in dir():\n",
    "    perplexity = torch.exp(manual_loss)\n",
    "    print(\"Perplexity: \", perplexity)\n",
    "    print(\"Vocabulary: \", GPT_CONFIG_124M[\"vocab_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5dcba",
   "metadata": {},
   "source": [
    "# Playground for Losses. Cross Entropy and Perplexity\n",
    "* Calculation steps for Cross Entropy\n",
    "* Calculation steps for Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285f56",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49d2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"06. GPTModel.ipynb\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M) # From 06. GPTModel.ipynb\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "generate_tokens = generate_tokens # From 06. GPTModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d79d",
   "metadata": {},
   "source": [
    "## Translation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2b01812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc2b43",
   "metadata": {},
   "source": [
    "## Untrained output\n",
    "* Generates max 10 new tokens based on input content batch of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7260a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape ( batch x token_ids ): torch.Size([2, 3])\n",
      "Input tokens:\n",
      " tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n",
      "\n",
      "Output shape:  torch.Size([2, 13])\n",
      "\n",
      "Batch outputs: \n",
      "  [0]: every effort moves HO Receiver latent Mitt unable cycling denote Python clears formations\n",
      "  [1]: I really like energ opacity Labor Fen!] lichAlien adequately: membership\n"
     ]
    }
   ],
   "source": [
    "if '__file__' not in dir(): \n",
    "    # Batch of 2 texts, both with 4 tokens each\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Create batch by stacking individual text encodings\n",
    "    text1 = text_to_token_ids(\"every effort moves\", tokenizer)  # 3 tokens\n",
    "    text2 = text_to_token_ids(\"I really like\", tokenizer)        # 3 tokens\n",
    "    \n",
    "    # Stack into batch (2, 3)\n",
    "    inputs = torch.cat([text1, text2], dim=0)\n",
    "    \n",
    "    print(\"Input shape ( batch x token_ids ):\", inputs.shape)\n",
    "    print(\"Input tokens:\\n\", inputs)\n",
    "\n",
    "    # Generate tokens for batch\n",
    "    token_ids = generate_tokens(\n",
    "        model=model,\n",
    "        token_IDs=inputs,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nOutput shape: \", token_ids.shape)\n",
    "    \n",
    "    # Decode each batch item using token_ids_to_text\n",
    "    print(\"\\nBatch outputs: \")\n",
    "    for i in range(token_ids.shape[0]):\n",
    "        text = token_ids_to_text(token_ids[i:i+1], tokenizer)\n",
    "        print(f\"  [{i}]: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c77208",
   "metadata": {},
   "source": [
    "## Real model logits output\n",
    "* Real output of the model are logits (unnormalized scores from output layer)\n",
    "* After applying softmax we get a distribution of probabilites over the whole vocabulary\n",
    "* Argmax on probabilities retuns most probable token_id (array index) for next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ce10456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape (batch_size, num_tokens, vocab_size):  torch.Size([2, 3, 50257])\n",
      "tensor([[[6.7266e-05, 2.2964e-05, 6.6790e-06,  ..., 9.3710e-06,\n",
      "          2.8003e-05, 1.3810e-05],\n",
      "         [3.7238e-05, 1.5398e-05, 9.9722e-06,  ..., 6.6796e-06,\n",
      "          1.2250e-05, 6.4735e-05],\n",
      "         [1.9261e-05, 1.1219e-05, 2.4156e-05,  ..., 1.5356e-05,\n",
      "          4.5120e-06, 3.8627e-05]],\n",
      "\n",
      "        [[2.2608e-05, 2.3807e-05, 1.7500e-05,  ..., 2.0317e-05,\n",
      "          2.4596e-05, 3.4503e-05],\n",
      "         [1.3748e-05, 2.5611e-05, 9.0395e-06,  ..., 1.1674e-05,\n",
      "          2.4658e-05, 7.0392e-05],\n",
      "         [4.1235e-05, 6.7255e-06, 1.4985e-05,  ..., 7.6617e-06,\n",
      "          7.1792e-06, 6.0584e-06]]])\n",
      "\n",
      "ArgMax -> Token IDs shape: torch.Size([2, 3])\n",
      "Token IDs:\n",
      " tensor([[24851,   406, 40115],\n",
      "        [29716, 40825, 19647]])\n",
      "\n",
      "Batch [0]:\n",
      "  'every' predicts -> 'etti' [24851]\n",
      "  'every effort' predicts -> ' L' [406]\n",
      "  'every effort moves' predicts -> ' HO' [40115]\n",
      "\n",
      "Batch [1]:\n",
      "  'I' predicts -> 'ovsky' [29716]\n",
      "  'I really' predicts -> 'Sounds' [40825]\n",
      "  'I really like' predicts -> ' energ' [19647]\n"
     ]
    }
   ],
   "source": [
    "if '__file__' not in dir(): \n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "\n",
    "    probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "    print(\"\\nOutput shape (batch_size, num_tokens, vocab_size): \", probas.shape) \n",
    "    print(probas)\n",
    "\n",
    "    token_ids = torch.argmax(probas, dim=-1)\n",
    "    print(\"\\nArgMax -> Token IDs shape:\", token_ids.shape)\n",
    "    print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "    # Show predictions for each batch item\n",
    "    for i in range(token_ids.shape[0]):\n",
    "        print(f\"\\nBatch [{i}]:\")\n",
    "        input_tokens = inputs[i].tolist()\n",
    "        predicted_tokens = token_ids[i].tolist()\n",
    "        \n",
    "        for pos in range(len(input_tokens)):\n",
    "            # Context: all tokens up to and including current position\n",
    "            context = tokenizer.decode(input_tokens[:pos+1])\n",
    "            predicted = tokenizer.decode([predicted_tokens[pos]])\n",
    "            predicted_id = predicted_tokens[pos]\n",
    "            print(f\"  '{context}' predicts -> '{predicted}' [{predicted_id}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1ce57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e5dcba",
   "metadata": {},
   "source": [
    "# Playground for Losses. Cross Entropy and Perplexity\n",
    "* Calculation steps for Cross Entropy\n",
    "* Calculation steps for Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285f56",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "49d2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "torch.set_printoptions(threshold=10, edgeitems=3, precision=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# See these files for details\n",
    "%run \"01. DataPreparation.ipynb\"\n",
    "%run \"06. GPTModel.ipynb\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "# From 01. DataPreparation\n",
    "create_dataloader = create_dataloader\n",
    "\n",
    "# From 06. GPTModel\n",
    "model = GPTModel(GPT_CONFIG_124M) # From 06. GPTModel.ipynb\n",
    "model.eval();  # Disable dropout during inference\n",
    "generate_tokens = generate_tokens # From 06. GPTModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68d79d",
   "metadata": {},
   "source": [
    "## Creating Input and Target Generation\n",
    "* Generates test Batches Input + Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a2b01812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Target shape: torch.Size([2, 4])\n",
      "\n",
      "Input tokens:\n",
      " tensor([[12925,  2250,   351,  1583],\n",
      "        [ 3947,   284,  1950,   257]])\n",
      "\n",
      "Target tokens:\n",
      " tensor([[2250,  351, 1583,   13],\n",
      "        [ 284, 1950,  257,  649]])\n",
      "\n",
      "Decoded inputs:\n",
      "  [0]:  countless hours with Dr\n",
      "  [1]:  seemed to suggest a\n",
      "\n",
      "Decoded targets:\n",
      "  [0]:  hours with Dr.\n",
      "  [1]:  to suggest a new\n"
     ]
    }
   ],
   "source": [
    "def get_test_batch(batch_size=2, context_length=4, stride=4, verbose=False):\n",
    "    \n",
    "    # Load sample text and create dataloader\n",
    "    with open(\"00. Robins Small Text Sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read()\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataloader = create_dataloader(\n",
    "        raw_text, \n",
    "        tokenizer_model_name=\"gpt2\",\n",
    "        batch_size=batch_size, \n",
    "        context_length=context_length, \n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    # Get one batch of input and target data\n",
    "    batch = next(iter(dataloader))\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        print(\"Target shape:\", targets.shape)\n",
    "        print(\"\\nInput tokens:\\n\", inputs)\n",
    "        print(\"\\nTarget tokens:\\n\", targets)\n",
    "        print(\"\\nDecoded inputs:\")\n",
    "        for i in range(inputs.shape[0]):\n",
    "            print(f\"  [{i}]: {tokenizer.decode(inputs[i].tolist())}\")\n",
    "        print(\"\\nDecoded targets:\")\n",
    "        for i in range(targets.shape[0]):\n",
    "            print(f\"  [{i}]: {tokenizer.decode(targets[i].tolist())}\")\n",
    "    \n",
    "    return inputs, targets, tokenizer\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    inputs, targets, tokenizer = get_test_batch(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c77208",
   "metadata": {},
   "source": [
    "## Untrained model logits output (single forward pass)\n",
    "* Real output of the model are logits (unnormalized scores from output layer)\n",
    "* After applying softmax we get a distribution of probabilites over the whole vocabulary (values sum to 1)\n",
    "* Argmax on probabilities retuns most probable token_id (array index) for next token\n",
    "* Model calculates logits for all input positions in parallel (Transformer architecture), but only the last position's logits are used for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1ce10456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape (batch_size, num_tokens, vocab_size): torch.Size([2, 4, 50257])\n",
      "\n",
      "Returns logits for every partial sequence. Current end token as Q decodes next output\n",
      "In production only last logit is relevant. In training all can be used\n",
      "\n",
      "Probability vector for next word after every of the 4 input token (batch 0).\n",
      "Index = Token_ID\n",
      " tensor([[1.92e-05, 4.03e-05, 1.61e-05,  ..., 2.57e-05, 2.05e-05, 1.55e-05],\n",
      "        [1.62e-05, 1.89e-05, 1.14e-05,  ..., 6.62e-06, 1.93e-05, 6.95e-05],\n",
      "        [1.81e-05, 2.26e-05, 1.77e-05,  ..., 1.26e-05, 9.82e-06, 1.43e-05],\n",
      "        [1.85e-05, 4.70e-05, 1.46e-05,  ..., 1.16e-05, 1.28e-05, 1.95e-05]])\n",
      "\n",
      "ArgMax gives Index of vector = token_id with max value. Shape: torch.Size([2, 4])\n",
      "Token IDs (all batches):\n",
      " tensor([[28155, 31548, 18617, 10345],\n",
      "        [21391, 24756, 43525, 26721]])\n",
      "\n",
      "Batch [0]:\n",
      "  ' countless' --> 'witz' [28155] | target ' hours' [2250] ✗\n",
      "  ' countless hours' --> ' dummy' [31548] | target ' with' [351] ✗\n",
      "  ' countless hours with' --> ' launches' [18617] | target ' Dr' [1583] ✗\n",
      "  ' countless hours with Dr' --> ' functional' [10345] | target '.' [13] ✗\n",
      "\n",
      "Batch [1]:\n",
      "  ' seemed' --> ' intentional' [21391] | target ' to' [284] ✗\n",
      "  ' seemed to' --> ' Charlottesville' [24756] | target ' suggest' [1950] ✗\n",
      "  ' seemed to suggest' --> 'Located' [43525] | target ' a' [257] ✗\n",
      "  ' seemed to suggest a' --> ' drills' [26721] | target ' new' [649] ✗\n"
     ]
    }
   ],
   "source": [
    "def analyze_predictions(model, inputs, targets, tokenizer, verbose=False):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.argmax(probas, dim=-1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nOutput shape (batch_size, num_tokens, vocab_size):\", probas.shape)\n",
    "        print(\"\\nReturns logits for every partial sequence. Current end token as Q decodes next output\\n\"\n",
    "              \"In production only last logit is relevant. In training all can be used\")\n",
    "        print(f'\\nProbability vector for next word after every of the {probas.shape[1]} input token (batch 0).\\nIndex = Token_ID\\n', probas[0])\n",
    "        print(\"\\nArgMax gives Index of vector = token_id with max value. Shape:\", predicted_ids.shape)\n",
    "        print(\"Token IDs (all batches):\\n\", predicted_ids)\n",
    "        \n",
    "        # Show predictions vs targets for each batch item\n",
    "        for i in range(predicted_ids.shape[0]):\n",
    "            print(f\"\\nBatch [{i}]:\")\n",
    "            input_tokens = inputs[i].tolist()\n",
    "            predicted_tokens = predicted_ids[i].tolist()\n",
    "            target_tokens = targets[i].tolist()\n",
    "            \n",
    "            for pos in range(len(input_tokens)):\n",
    "                context = tokenizer.decode(input_tokens[:pos+1])\n",
    "                predicted = tokenizer.decode([predicted_tokens[pos]])\n",
    "                target = tokenizer.decode([target_tokens[pos]])\n",
    "                match = \"✓\" if predicted_tokens[pos] == target_tokens[pos] else \"✗\"\n",
    "                print(f\"  '{context}' --> '{predicted}' [{predicted_tokens[pos]}] | target '{target}' [{target_tokens[pos]}] {match}\")\n",
    "    \n",
    "    return logits, probas, predicted_ids\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    logits, probas, predicted_ids = analyze_predictions(model, inputs, targets, tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1ce57",
   "metadata": {},
   "source": [
    "# Training Goal and Cross Entropy Loss\n",
    "* Goal: Maximize probability of correct target tokens (bring `probas[target_id]` close to 1.0)\n",
    "* Logarithms convert products to sums and prevent numerical underflow, making optimization easier\n",
    "* Cross-Entropy Loss = negative average log probability of target tokens: `-mean(log(probas[target_id]))`\n",
    "* Loss can be computed at different granularities:\n",
    "    * Per-token: `-log(P(y_i))` for individual predictions (batch_size, context_length)\n",
    "    * Per-sequence: Average over tokens in each sequence (batch_size,)\n",
    "    * Batch-average (common implemented here): Single scalar averaged over all predictions (N = batch_size × context_length)\n",
    "* Formula: `Loss = -1/N * Σ log(P(y_i))`\n",
    "    * `N` = total number of predictions being averaged over\n",
    "    * `y_i` = the i-th target token (correct token at position i)\n",
    "    * `P(y_i)` = model's predicted probability for target token y_i\n",
    "    * `Σ` = sum over all N predictions\n",
    "* Training typically uses batch-average loss for backpropagation\n",
    "* Lower loss = model assigns higher probability to correct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3aab44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log of probabilities (batch 0):\n",
      " tensor([[-10.86, -10.12, -11.03,  ..., -10.57, -10.80, -11.08],\n",
      "        [-11.03, -10.87, -11.38,  ..., -11.93, -10.86,  -9.57],\n",
      "        [-10.92, -10.70, -10.94,  ..., -11.28, -11.53, -11.16],\n",
      "        [-10.90,  -9.97, -11.14,  ..., -11.37, -11.27, -10.84]])\n",
      "\n",
      "Log probabilities for target tokens only:\n",
      "Shape: torch.Size([2, 4])\n",
      "tensor([[-11.64, -11.54, -11.44, -10.46],\n",
      "        [-10.09, -10.37, -11.06, -10.79]])\n",
      "\n",
      "Average log probability: -10.9250\n",
      "Negative average log probability (loss): 10.9250\n",
      "\n",
      "Per-token log probabilities for targets:\n",
      "\n",
      "Batch [0]:\n",
      "  Position 0: target ' hours' [2250] -> log_prob: -11.6440\n",
      "  Position 1: target ' with' [351] -> log_prob: -11.5365\n",
      "  Position 2: target ' Dr' [1583] -> log_prob: -11.4386\n",
      "  Position 3: target '.' [13] -> log_prob: -10.4596\n",
      "\n",
      "Batch [1]:\n",
      "  Position 0: target ' to' [284] -> log_prob: -10.0913\n",
      "  Position 1: target ' suggest' [1950] -> log_prob: -10.3741\n",
      "  Position 2: target ' a' [257] -> log_prob: -11.0634\n",
      "  Position 3: target ' new' [649] -> log_prob: -10.7929\n"
     ]
    }
   ],
   "source": [
    "def extract_target_log_probas(probas, targets, tokenizer, verbose=False):\n",
    "    # Compute logarithm of all token probabilities\n",
    "    log_probas = torch.log(probas)\n",
    "    \n",
    "    # Extract log probabilities only for target tokens\n",
    "    batch_size, num_tokens = targets.shape\n",
    "    target_log_probas = log_probas[torch.arange(batch_size).unsqueeze(1), torch.arange(num_tokens), targets]\n",
    "    \n",
    "    # Calculate the average log probability for target tokens\n",
    "    avg_log_probas = torch.mean(target_log_probas)\n",
    "    \n",
    "    # Negative average log probability (cross-entropy loss)\n",
    "    cross_entropy_loss = avg_log_probas * -1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Log of probabilities (batch 0):\\n\", log_probas[0])\n",
    "        print(\"\\nLog probabilities for target tokens only:\")\n",
    "        print(\"Shape:\", target_log_probas.shape)\n",
    "        print(target_log_probas)\n",
    "        print(f\"\\nAverage log probability: {avg_log_probas.item():.4f}\")\n",
    "        print(f\"Negative average log probability (loss): {cross_entropy_loss.item():.4f}\")\n",
    "        \n",
    "        # Show per-token breakdown\n",
    "        print(\"\\nPer-token log probabilities for targets:\")\n",
    "        for i in range(batch_size):\n",
    "            print(f\"\\nBatch [{i}]:\")\n",
    "            for pos in range(num_tokens):\n",
    "                target_token = targets[i, pos].item()\n",
    "                log_prob = target_log_probas[i, pos].item()\n",
    "                target_text = tokenizer.decode([target_token])\n",
    "                print(f\"  Position {pos}: target '{target_text}' [{target_token}] -> log_prob: {log_prob:.4f}\")\n",
    "    \n",
    "    return target_log_probas, avg_log_probas, cross_entropy_loss\n",
    "\n",
    "if '__file__' not in dir():\n",
    "    target_log_probas, avg_log_probas, cross_entropy_loss = extract_target_log_probas(probas, targets, tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328cc08",
   "metadata": {},
   "source": [
    "## Use Pytorch Calculation function\n",
    "* Takes care of softmax, log-probability etc.\n",
    "* Batch dimensions need to be flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f68774de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy loss: 10.9250\n",
      "PyTorch cross-entropy loss: 10.9250\n"
     ]
    }
   ],
   "source": [
    "# Use Pytorch Calculation function\n",
    "if '__file__' not in dir():\n",
    "    # Manual calculation\n",
    "    _, _, manual_loss = extract_target_log_probas(probas, targets, tokenizer, verbose=False)\n",
    "    \n",
    "    # PyTorch CrossEntropyLoss\n",
    "    # Flatten: (batch, context, vocab) -> (batch*context, vocab) and (batch, context) -> (batch*context)\n",
    "    logits_flat = logits.flatten(0, 1)  # Flatten first 2 dims (start, end)\n",
    "    targets_flat = targets.flatten()     # Flatten all dims\n",
    "    pytorch_loss = nn.CrossEntropyLoss()(logits_flat, targets_flat)\n",
    "    \n",
    "    print(f\"Manual cross-entropy loss: {manual_loss.item():.4f}\")\n",
    "    print(f\"PyTorch cross-entropy loss: {pytorch_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f18ae",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "* Simply exponential of cross-entropy loss\n",
    "* Considered more interpratable \n",
    "    * Can be understood as en estimate of which numbers of vocabulary a model is uncertain about\n",
    "    * How well the probability predicted by model matches actual distribution in words of dataset\n",
    "    * Lower perplexity -> better model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5399b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  tensor(55550.56)\n",
      "Vocabulary:  50257\n"
     ]
    }
   ],
   "source": [
    "if '__file__' not in dir():\n",
    "    perplexity = torch.exp(manual_loss)\n",
    "    print(\"Perplexity: \", perplexity)\n",
    "    print(\"Vocabulary: \", GPT_CONFIG_124M[\"vocab_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Playground for Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23c5a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run previous notebook to have data prepared\n",
    "%run \"./DataPreparation.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Prototype Attention Head\n",
    "Implementation of a single attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1453b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # init weight matrices for input -> query, key, weight projection\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        \n",
    "        # init dropout (prevent overfitting)\n",
    "        self.dropout = nn.Dropout(dropout) # dropout is a probability\n",
    "        \n",
    "        # registers static causal masking matrix (diagonal) in same device as model (is self.causal_mask)\n",
    "        self.register_buffer('causal_mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # Causal Masking on diagonal triangle\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # stores dimensions of input (here only context length needed)\n",
    "        batch_size, con_len, in_dim = x.shape # set them to the shape of x ( 8, 4, 256 )\n",
    "        \n",
    "        # using the weight matrices for projection of the input to query, key and value\n",
    "        # broadcasting (for badges)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # calculates all attention scores\n",
    "        # Compares query for every input token to all keys\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # creates bool mask (diagonal) fills all causal invisible elements with -inf\n",
    "        attn_scores.masked_fill( self.causal_mask.bool()[:con_len, :con_len], -torch.inf)\n",
    "\n",
    "        # calculations of soft max scaled with sqrt of keys length dimension (keys.shape)\n",
    "        # note: dim=-1 takes most internal dimension in an array \n",
    "        # example: [ [ [ 1, 2, 3] ],  [ [ 4, 5, 6 ], [ 7, 8, 8 ] ] ] -> 3\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # applying dropout to weights\n",
    "        # randomly sets weights to 0 based on probability\n",
    "        # !! scales remaining values up to compensate dropped weights\n",
    "        attn_weights = self.dropout(attn_weights) # Additional random dropout\n",
    "\n",
    "        # sum of attention weighted values for every input (including causal masked knowledge)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f117a2",
   "metadata": {},
   "source": [
    "## Short insight in masking and causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4820aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matrix (ex. attention_scores):\n",
      " tensor([[0.3093, 0.8304, 0.8787, 0.5409],\n",
      "        [0.9386, 0.2404, 0.3011, 0.7845],\n",
      "        [0.8350, 0.5887, 0.7426, 0.7358],\n",
      "        [0.3101, 0.9291, 0.5029, 0.2398]])\n",
      "\n",
      "mask (buffered causal_mask of size context_length):\n",
      " tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "mask.bool():\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "mask.bool()[interval] (clipped to match input)\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "matrix.mask_fill(mask.bool()[interval], value)\n",
      " tensor([[0.3093,   -inf,   -inf,   -inf],\n",
      "        [0.9386, 0.2404,   -inf,   -inf],\n",
      "        [0.8350, 0.5887, 0.7426,   -inf],\n",
      "        [0.3101, 0.9291, 0.5029, 0.2398]])\n"
     ]
    }
   ],
   "source": [
    "# Triangular Matrix filtering example for causal masking\n",
    "def mask_demo():\n",
    "\n",
    "    # matrix to mask. Example attention scores\n",
    "    matrix = torch.rand(4,4)\n",
    "\n",
    "    # buffered static causal_mask\n",
    "    mask = torch.triu(torch.ones(4, 4), diagonal=1)\n",
    "\n",
    "    print(\"\\nmatrix (ex. attention_scores):\\n\", matrix) # some value matrix\n",
    "    print(\"\\nmask (buffered causal_mask of size context_length):\\n\", mask) # triangle mask matrix consisting of 0 and 1\n",
    "    print(\"\\nmask.bool():\\n\", mask.bool()) # makes matrix with 0 and 1 to true and false\n",
    "    print(\"\\nmask.bool()[interval] (clipped to match input)\\n\", mask.bool()[:3, :3]) # sub matrix\n",
    "    print(\"\\nmatrix.mask_fill(mask.bool()[interval], value)\\n\", matrix.masked_fill(mask.bool()[:4, :4], -torch.inf))\n",
    "mask_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a790",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "An implementation of Multi Head Attention\n",
    "Multiple heads train themself on parts of the input vector. They represent a part of the query, key, value (short qkv) matrices and are combined in the end by linear layer to context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d4c94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, num_head, qkv_bias=False, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check if output dimension is dividable by attention head number without rest\n",
    "        # Input is split to num_head chunks of length head_dim\n",
    "        assert output_dim % num_head == 0, \"Output dimension must be dividable by head_num\"\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_head\n",
    "        self.head_dim = output_dim // num_head\n",
    "\n",
    "        # Init weight matrices for input to qkv projection (full not yet separated into head parts)\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "\n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.out_proj = nn.Linear(output_dim, output_dim) # Linear layer for combination of head outputs\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"input_dim =\", input_dim)\n",
    "            print(\"output_dim =\", output_dim)\n",
    "            print(\"num_heads =\", num_head)\n",
    "            print(\"head_dim =\", output_dim)\n",
    "            print(f\"generating nn.Linear({input_dim}, {output_dim}) weights for query, key and value\")\n",
    "            print(f\"generating causal diagonal mask torch.triu(torch.ones({context_length}, {context_length}), diagonal=1) for causal masking of attn_scores\")        \n",
    "            print(f\"generating dropout nn.Dropout({dropout}) for random dropout of attn_weights\")        \n",
    "            print(f\"generating optional nn.Linear({output_dim}, {output_dim}) weights for final context_vector projection\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        # using weight matrices for projection of the input to qkv (not yet splitted for attention heads)\n",
    "        # broadcasting for badges\n",
    "        # -> shape: batch_size, context_length, output_dim\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Implicitly splitting matrix by adding head_num dimension\n",
    "        # Unrol last dim: (batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
    "        # Example ( 8, 4, 256 ) -> ( 8, 4, 8, 32 ) for num_heads = 8 and head_dim = 32\n",
    "        queries = queries.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to use for query comparison - move num_head to front\n",
    "        # (batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        # Compute scaled dot_production attention\n",
    "        attn_scores = queries @ keys.transpose(2,3) # Dot product for each head\n",
    "\n",
    "        # Causal Masking\n",
    "        mask_bool = self.causal_mask.bool()[:context_length, :context_length]\n",
    "        attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim =-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # build context vector switch back num_heads and context_length, combining heads\n",
    "        # self.output_dim = self.num_heads * self.head_dim\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, context_length, self.output_dim)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection by Linear layer\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b733d",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8420924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "input_dim = 756\n",
      "output_dim = 128\n",
      "num_heads = 8\n",
      "head_dim = 128\n",
      "generating nn.Linear(756, 128) weights for query, key and value\n",
      "generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "\n",
      "\n",
      "----- Using real data input (see DataPreparation.ipynb) --\n",
      "\n",
      "\n",
      "Init test embedder ...\n",
      "Generating token_embeddings (50252 x 756)\n",
      "Generating pos_embeddings (4 x 756)\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "First batch elements Input x:\n",
      " tensor([   13, 26190,    11, 31174]) . Sebastian, debating\n",
      "\n",
      "First batch elements Target y:\n",
      " tensor([26190,    11, 31174, 12779])  Sebastian, debating possibilities\n",
      "\n",
      "embeddings[0] for x (4 x 756):\n",
      " tensor([[ 1.0139, -0.1002,  2.0504,  ...,  0.8746, -1.4289,  0.5435],\n",
      "        [-0.1608, -0.5881,  0.7377,  ...,  0.5694,  0.1068, -1.2421],\n",
      "        [ 0.8515, -0.8154, -1.1777,  ..., -0.2011, -1.1755, -1.1739],\n",
      "        [ 0.8964,  1.2855,  0.0168,  ...,  0.1689, -0.9626, -0.2634]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "pos_embeddings[0] (4 x 756):\n",
      " tensor([[-0.2388,  0.6405,  1.0115,  ...,  0.5660, -0.0552,  1.1899],\n",
      "        [ 0.7640,  1.1029,  1.5906,  ...,  0.5649, -0.3157,  1.9596],\n",
      "        [-0.7988,  1.0535,  2.1774,  ..., -0.7581,  2.4189,  0.0540],\n",
      "        [ 0.6713, -0.3531, -1.3903,  ...,  0.6987,  0.2906, -0.4310]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input_embeddings[0] = embeddings[0] + pos_embeddings[0]:\n",
      " tensor([[ 0.7751,  0.5402,  3.0619,  ...,  1.4406, -1.4841,  1.7333],\n",
      "        [ 0.6032,  0.5149,  2.3282,  ...,  1.1344, -0.2089,  0.7175],\n",
      "        [ 0.0527,  0.2381,  0.9997,  ..., -0.9592,  1.2434, -1.1198],\n",
      "        [ 1.5677,  0.9324, -1.3736,  ...,  0.8676, -0.6719, -0.6945]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 756])\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      " Shape of context_vector:\n",
      " torch.Size([8, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "def multi_head_attention_test_run(verbose=False):\n",
    "    if verbose: print(\"\\n\\n------- initializing Multi Head Attention ----------------\\n\")\n",
    "    mha = MultiHeadAttention(input_dim=756, output_dim=128, context_length=4, dropout=0.2, num_head=8, verbose=True)\n",
    "\n",
    "    if verbose: print(\"\\n\\n----- Using real data input (see DataPreparation.ipynb) --\\n\")\n",
    "    batch = get_test_input_embedding(verbose=verbose) # defined in DataPreparation.ipynb\n",
    "\n",
    "    if verbose: print(\"\\n\\n------- performing multi head attention ------------------\\n\")\n",
    "    context_vector = mha(batch)\n",
    "\n",
    "    if verbose: print(\"\\n Shape of context_vector:\\n\", context_vector.shape)\n",
    "\n",
    "\n",
    "_test_run = multi_head_attention_test_run(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d0f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

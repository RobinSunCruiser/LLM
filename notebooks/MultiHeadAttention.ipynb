{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f964f10f",
   "metadata": {},
   "source": [
    "# Creating some playground\n",
    "* Dataset\n",
    "* Dataloader\n",
    "* Multihead-Attention\n",
    "\n",
    "\n",
    "## Dataset\n",
    "  Reprocesses the entire dataset into training pairs, and `__getitem__`\n",
    "   just retrieves them by index.\n",
    "\n",
    "  Process:\n",
    "  1. Tokenize text → convert to token IDs\n",
    "  2. Sliding window → extract sequences of max_length + 1\n",
    "  3. Split each sequence → input `[i:max_length]` and target `[i+1:max_length+1]` (shifted by 1)\n",
    "  4. Store as pairs → `self.input_ids[i]` and `self.target_ids[i]` as a training pair\n",
    "  5. Jumps `stride` width\n",
    "\n",
    "  Example:\n",
    "  sequence = [1, 2, 3, 4, 5]  # length 5, max_length = 4\n",
    "  # Split into:\n",
    "  input_ids  = [1, 2, 3, 4]   # predict next token at each position\n",
    "  target_ids = [2, 3, 4, 5]   # what should be predicted\n",
    "\n",
    "  Training pairs at same index:\n",
    "  - input_ids[0] → predict → target_ids[0]\n",
    "  - input_ids[1] → predict → target_ids[1]\n",
    "  - etc.\n",
    "\n",
    "  This preprocessing in __init__ makes __getitem__ very fast since it just returns pre-computed pairs.\n",
    "  The model learns to predict the next token at each position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b87417d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # tokenization library openai\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize txt\n",
    "        token_ids = tokenizer.encode( txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        # Walks throught IDs and prepaires training sets for every index of input_ids and target_ids\n",
    "        # Stride determines the jump wide for one loop\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ffd25",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Wrapper for initialization and data loader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad535e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, tokenizer_model_name=\"gpt2\", batch_size=4, max_length=256, stride=128, shuffle=True):\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_model_name)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23e248",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Loading of file Robins Small Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9c144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Robins Small Text Sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataloader = create_dataloader( raw_text, tokenizer_model_name=\"gpt2\", batch_size=8, max_length=4, stride=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cfa36",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "Gets input embeddings from token_ids\n",
    "* Create embedding layers for vocabulary and embedding dimensions\n",
    "* Positional encoding from embedding layer with context_length and embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "345df833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying first row of batch\n",
      "\n",
      "Input x:\n",
      " tensor([2506,  326, 3360,   11])  everyone that sometimes,\n",
      "\n",
      "Target y:\n",
      " tensor([ 326, 3360,   11,  262])  that sometimes, the\n",
      "\n",
      " embeddings for x:\n",
      " tensor([[ 0.9752,  0.3436, -1.0103,  ...,  0.9809,  1.2331, -0.1515],\n",
      "        [-0.2354, -0.6906, -0.7542,  ..., -2.3173, -1.1541, -1.0304],\n",
      "        [-0.5122, -0.5704, -0.9882,  ..., -0.7218, -1.0930, -1.0003],\n",
      "        [ 1.8313, -0.6159, -0.6073,  ..., -2.3891,  0.7178, -1.5831]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      " pos_embeddings for 0-3:\n",
      " tensor([[-2.9261, -2.8944,  0.7488,  ...,  1.0027,  0.7249, -0.1917],\n",
      "        [ 0.0056, -0.5059,  0.5341,  ..., -0.7495,  1.3472, -0.8115],\n",
      "        [ 0.5100,  0.1452, -1.1741,  ...,  0.5613,  1.2736, -1.4704],\n",
      "        [ 1.1651, -1.2304, -0.9989,  ...,  0.4287, -0.3611,  0.4499]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      " input_embeddings = embeddings + pos_embeddings:\n",
      " tensor([[-1.9509e+00, -2.5508e+00, -2.6152e-01,  ...,  1.9836e+00,\n",
      "          1.9580e+00, -3.4329e-01],\n",
      "        [-2.2974e-01, -1.1964e+00, -2.2012e-01,  ..., -3.0667e+00,\n",
      "          1.9312e-01, -1.8419e+00],\n",
      "        [-2.2381e-03, -4.2520e-01, -2.1624e+00,  ..., -1.6051e-01,\n",
      "          1.8060e-01, -2.4707e+00],\n",
      "        [ 2.9964e+00, -1.8463e+00, -1.6062e+00,  ..., -1.9605e+00,\n",
      "          3.5674e-01, -1.1332e+00]], grad_fn=<SelectBackward0>)\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9509e+00, -2.5508e+00, -2.6152e-01,  ...,  1.9836e+00,\n",
       "           1.9580e+00, -3.4329e-01],\n",
       "         [-2.2974e-01, -1.1964e+00, -2.2012e-01,  ..., -3.0667e+00,\n",
       "           1.9312e-01, -1.8419e+00],\n",
       "         [-2.2381e-03, -4.2520e-01, -2.1624e+00,  ..., -1.6051e-01,\n",
       "           1.8060e-01, -2.4707e+00],\n",
       "         [ 2.9964e+00, -1.8463e+00, -1.6062e+00,  ..., -1.9605e+00,\n",
       "           3.5674e-01, -1.1332e+00]],\n",
       "\n",
       "        [[-3.5598e+00, -2.7543e+00, -5.7565e-01,  ...,  1.3383e+00,\n",
       "           1.5547e+00, -8.4338e-01],\n",
       "         [ 1.8369e+00, -1.1218e+00, -7.3182e-02,  ..., -3.1386e+00,\n",
       "           2.0650e+00, -2.3946e+00],\n",
       "         [ 1.8134e+00, -1.7097e-01, -1.9379e+00,  ...,  4.9678e-01,\n",
       "           1.7161e+00,  8.8213e-02],\n",
       "         [ 2.1793e+00, -9.5127e-02,  9.3677e-02,  ...,  7.4319e-01,\n",
       "          -1.0008e+00,  1.5751e+00]],\n",
       "\n",
       "        [[-2.9704e+00, -3.0695e+00,  2.1067e+00,  ...,  1.3326e+00,\n",
       "           1.9507e+00,  1.5052e-01],\n",
       "         [ 1.8369e+00, -1.1218e+00, -7.3182e-02,  ..., -3.1386e+00,\n",
       "           2.0650e+00, -2.3946e+00],\n",
       "         [-4.2972e-01,  1.2521e+00, -1.2592e+00,  ...,  1.3542e+00,\n",
       "           1.6807e-02, -1.0450e+00],\n",
       "         [ 1.4073e+00, -1.4823e+00, -3.8331e-02,  ...,  6.1426e-01,\n",
       "          -7.3223e-02, -5.8687e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4117e+00, -3.1397e+00,  1.2803e+00,  ...,  1.8296e+00,\n",
       "           1.3995e+00, -4.4736e-01],\n",
       "         [-3.0833e-01, -1.4111e+00,  2.1171e+00,  ..., -2.2399e-01,\n",
       "           6.5674e-01, -1.0406e+00],\n",
       "         [ 6.4843e-01, -1.3220e-01, -1.7268e+00,  ...,  8.3844e-01,\n",
       "           1.6907e+00, -9.3823e-01],\n",
       "         [ 2.0430e+00, -6.7845e-01, -2.1087e+00,  ...,  1.5379e+00,\n",
       "          -2.3931e-01,  3.7634e-01]],\n",
       "\n",
       "        [[-4.1538e+00, -3.0951e+00,  9.4969e-02,  ...,  1.7882e+00,\n",
       "           1.6533e+00, -4.9347e-01],\n",
       "         [ 1.8369e+00, -1.1218e+00, -7.3182e-02,  ..., -3.1386e+00,\n",
       "           2.0650e+00, -2.3946e+00],\n",
       "         [ 1.2178e+00, -1.6528e-01, -2.0604e+00,  ...,  1.0250e+00,\n",
       "           7.3745e-01, -7.6701e-01],\n",
       "         [ 5.9277e-01, -1.8574e+00, -6.9902e-01,  ...,  1.0029e-01,\n",
       "          -3.0596e-01,  5.9128e-03]],\n",
       "\n",
       "        [[-3.9890e+00, -3.9081e+00,  1.7941e+00,  ..., -1.7933e-01,\n",
       "           1.6068e+00,  9.0098e-01],\n",
       "         [ 9.1787e-01,  1.4947e+00,  5.9401e-01,  ..., -1.2426e+00,\n",
       "           1.5910e+00, -8.5758e-01],\n",
       "         [-1.4964e+00,  7.0727e-02, -2.8779e-01,  ...,  3.2562e-01,\n",
       "           1.5336e+00, -9.6138e-02],\n",
       "         [ 2.5803e-01, -3.1318e+00, -2.1668e+00,  ...,  1.3612e-01,\n",
       "          -5.9466e-01,  3.9148e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_embedding():\n",
    "\n",
    "    vocab_size = 50252\n",
    "    embedding_dim = 256\n",
    "    con_len = 4\n",
    "    positions = torch.arange(con_len) # tensor([0, 1, 2, 3])\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # for debuggin outputs\n",
    "\n",
    "    token_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "    pos_embedding_layer = nn.Embedding(con_len, embedding_dim)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        print(\"Displaying first row of batch\")\n",
    "        print(\"\\nInput x:\\n\", x[0], tokenizer.decode(x[0].tolist()))\n",
    "        print(\"\\nTarget y:\\n\", y[0], tokenizer.decode(y[0].tolist()))\n",
    "\n",
    "        embeddings = token_embedding_layer(x)\n",
    "        pos_embeddings = pos_embedding_layer(positions)\n",
    "\n",
    "        print(\"\\n embeddings for x:\\n\", embeddings[0])\n",
    "        print(\"\\n pos_embeddings for 0-3:\\n\", pos_embeddings)\n",
    "\n",
    "        input_embeddings = embeddings + pos_embeddings\n",
    "\n",
    "        print(\"\\n input_embeddings = embeddings + pos_embeddings:\\n\", input_embeddings[0])\n",
    "        print(\"Shape for input_embeddings: batch, context, embedding_dim \", input_embeddings.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "    return input_embeddings\n",
    "\n",
    "get_input_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb2765",
   "metadata": {},
   "source": [
    "## Prototype Attention Head\n",
    "Implementation of a simple multi attention head (no summed and split matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1453b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # dropout is a probability\n",
    "        self.register_buffer('causal_mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # Causal Masking on diagonal triangle\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, con_len, in_dim = x.shape # set them to the shape of x ( 8, 4, 256 )\n",
    "        \n",
    "        # Projection of the input to qkv using weight matrices and broadcasting\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)    # Calculation of attention scores (comparing query and key)\n",
    "        attn_scores.masked_fill( self.causal_mask.bool()[:con_len, :con_len], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) # Dimensions -1 for taking internal rows (in batch and context)\n",
    "        attn_weights = self.dropout(attn_weights) # Additional random dropout\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4820aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matrix:\n",
      " tensor([[0.0535, 0.5146, 0.2969, 0.1282],\n",
      "        [0.8037, 0.2223, 0.9951, 0.7353],\n",
      "        [0.0512, 0.8887, 0.2361, 0.5592],\n",
      "        [0.7751, 0.3303, 0.5652, 0.8721]])\n",
      "\n",
      "mask\n",
      " tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "mask.bool()\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "mask.bool()[interval]\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "\n",
      "matrix.mask_fill(mask.bool()[interval], value)\n",
      " tensor([[0.0535,   -inf,   -inf,   -inf],\n",
      "        [0.8037, 0.2223,   -inf,   -inf],\n",
      "        [0.0512, 0.8887, 0.2361,   -inf],\n",
      "        [0.7751, 0.3303, 0.5652, 0.8721]])\n"
     ]
    }
   ],
   "source": [
    "# Triangular Matrix filtering example for causal masking\n",
    "def mask_demo():\n",
    "    matrix = torch.rand(4,4)\n",
    "    mask = torch.triu(torch.ones(4, 4), diagonal=1)\n",
    "\n",
    "    print(\"\\nmatrix:\\n\", matrix) # some value matrix\n",
    "    print(\"\\nmask\\n\", mask) # triangle mask matrix consisting of 0 and 1\n",
    "    print(\"\\nmask.bool()\\n\", mask.bool()) # makes matrix with 0 and 1 to true and false\n",
    "    print(\"\\nmask.bool()[interval]\\n\", mask.bool()[:3, :3]) # sub matrix\n",
    "    print(\"\\nmatrix.mask_fill(mask.bool()[interval], value)\\n\", matrix.masked_fill(mask.bool()[:4, :4], -torch.inf))\n",
    "mask_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a790",
   "metadata": {},
   "source": [
    "## Prototype Multi Head Attention\n",
    "An implementation of Multi Head Attention\n",
    "Multiple heads train themself on parts of the input vector. They represent a part of the qkv matrices and are combined in the end by linear layer to context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d4c94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, context_length, dropout, num_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check if output dimension is dividable by attention head number\n",
    "        assert output_dim % num_head == 0, \"Output dimension must be dividable by head_num\"\n",
    "\n",
    "        print(\"- input_dim =\", input_dim)\n",
    "        print(\"- output_dim =\", output_dim)\n",
    "        print(\"- num_heads =\", num_head)\n",
    "        print(\"- head_dim =\", output_dim)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_head\n",
    "        self.head_dim = output_dim // num_head\n",
    "\n",
    "        # Init weight matrices for input to qkv projection (full not yet separated into head parts)\n",
    "        print(f\"- generating nn.Linear({input_dim}, {output_dim}) weights for query, key and value\")\n",
    "        self.W_query = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_key = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "        self.W_value = nn.Linear(input_dim, output_dim, qkv_bias)\n",
    "\n",
    "        print(f\"- generating causal diagonal mask torch.triu(torch.ones({context_length}, {context_length}), diagonal=1) for causal masking of attn_scores\")        \n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        print(f\"- generating dropout nn.Dropout({dropout}) for random dropout of attn_weights\")        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        print(f\"- generating optional nn.Linear({output_dim}, {output_dim}) weights for final context_vector projection\")\n",
    "        self.out_proj = nn.Linear(output_dim, output_dim) # Linear layer for combination of head outputs\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # local variables for input shape\n",
    "        batch_size, context_length, input_dim = x.shape\n",
    "\n",
    "        # Projection of the input to qkv using weight matrices and broadcasting for all attention heads together\n",
    "        # -> shape: batch_size, context_length, output_dim\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Implicitly splitting matrix by adding head_num dimension\n",
    "        # Unrol last dim: (batch_size, context_length, output_dim) -> (batch_size, context_length, head_num, head_dim)\n",
    "        # Example ( 8, 4, 256 ) -> ( 8, 4, 8, 32 ) for num_heads = 8 and head_dim = 32\n",
    "        queries = queries.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to use for query comparison - move num_head to front\n",
    "        # (batch_size, context_length, num_heads, head_dim) -> (batch_size, num_head, context_length, head_dim)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        # Compute scaled dot_production attention\n",
    "        attn_scores = queries @ keys.transpose(2,3) # Dot product for each head\n",
    "\n",
    "        # Causal Masking\n",
    "        mask_bool = self.causal_mask.bool()[:context_length, :context_length]\n",
    "        attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim =-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # build context vector switch back num_heads and context_length, combining heads\n",
    "        # self.output_dim = self.num_heads * self.head_dim\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, context_length, self.output_dim)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection by Linear layer\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b733d",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8420924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- initializing Multi Head Attention ----------------\n",
      "\n",
      "- input_dim = 256\n",
      "- output_dim = 128\n",
      "- num_heads = 8\n",
      "- head_dim = 128\n",
      "- generating nn.Linear(256, 128) weights for query, key and value\n",
      "- generating causal diagonal mask torch.triu(torch.ones(4, 4), diagonal=1) for causal masking of attn_scores\n",
      "- generating dropout nn.Dropout(0.2) for random dropout of attn_weights\n",
      "- generating optional nn.Linear(128, 128) weights for final context_vector projection\n",
      "\n",
      "\n",
      "------- generating input ---------------------------------\n",
      "\n",
      "Displaying first row of batch\n",
      "\n",
      "Input x:\n",
      " tensor([ 2627,   257, 34538,   286])  became a beacon of\n",
      "\n",
      "Target y:\n",
      " tensor([  257, 34538,   286, 11044])  a beacon of innovation\n",
      "\n",
      " embeddings for x:\n",
      " tensor([[ 0.8057, -0.6795, -0.4887,  ..., -0.3833, -0.0600, -0.3964],\n",
      "        [ 0.4135, -0.8302,  0.8324,  ..., -1.9796, -0.2031, -0.8872],\n",
      "        [ 0.8406,  0.0906, -0.0602,  ...,  0.1790, -1.3455, -0.3669],\n",
      "        [-1.3623, -0.9489,  0.2503,  ..., -1.1597,  0.1812,  1.2424]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      " pos_embeddings for 0-3:\n",
      " tensor([[ 1.1435, -0.1014,  1.5057,  ...,  0.8235, -1.1988, -0.2648],\n",
      "        [-0.3690,  0.4896,  0.4510,  ..., -0.8038,  0.3464, -0.2497],\n",
      "        [ 0.4060, -0.6073, -1.8696,  ...,  1.4453,  0.5371, -0.4359],\n",
      "        [ 0.3291, -1.6909, -0.2640,  ...,  0.6090, -1.2019,  0.1243]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      " input_embeddings = embeddings + pos_embeddings:\n",
      " tensor([[ 1.9492, -0.7809,  1.0169,  ...,  0.4402, -1.2588, -0.6613],\n",
      "        [ 0.0445, -0.3405,  1.2834,  ..., -2.7834,  0.1433, -1.1370],\n",
      "        [ 1.2466, -0.5167, -1.9297,  ...,  1.6244, -0.8084, -0.8028],\n",
      "        [-1.0331, -2.6399, -0.0137,  ..., -0.5506, -1.0207,  1.3667]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Shape for input_embeddings: batch, context, embedding_dim  torch.Size([8, 4, 256])\n",
      "\n",
      "\n",
      "------- performing multi head attention ------------------\n",
      "\n",
      "\n",
      " Shape of context_vector:\n",
      " torch.Size([8, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n------- initializing Multi Head Attention ----------------\\n\")\n",
    "mha = MultiHeadAttention(input_dim=256, output_dim=128, context_length=4, dropout=0.2, num_head=8 )\n",
    "\n",
    "print(\"\\n\\n------- generating input ---------------------------------\\n\")\n",
    "batch = get_input_embedding()\n",
    "\n",
    "print(\"\\n\\n------- performing multi head attention ------------------\\n\")\n",
    "context_vector = mha(batch)\n",
    "\n",
    "print(\"\\n Shape of context_vector:\\n\", context_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d0f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
